{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402c26c3",
   "metadata": {},
   "source": [
    "lora 어댑터와 기존 프루닝된 모델 업로드 후 결합하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a79c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from model_loader import load_pruned_model\n",
    "\n",
    "model_path = \"/pruned_activation\"\n",
    "base_model, tokenizer = load_pruned_model(\n",
    "    model_path,\n",
    "    device=torch.device(\"cuda\")\n",
    ")\n",
    "full_model = PeftModel.from_pretrained(base_model, \"/lora_tomato/checkpoint-921\")\n",
    "\n",
    "full_model = full_model.merge_and_unload()\n",
    "\n",
    "# full model 저장\n",
    "full_model.save_pretrained(\"/full_model\")\n",
    "tokenizer.save_pretrained(\"/full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59955c80",
   "metadata": {},
   "source": [
    "gguf 파일 생성 및 4bit 양자화 적용 (로컬에서는 아래 명령어 느낌표 제거 후 터미널에서 순차적으로 진행하면 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df43a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# gguf 파일 생성\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "%cd /content/llama.cpp\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "!mkdir -p build\n",
    "!cmake -B build\n",
    "!cmake --build build\n",
    "\n",
    "!python convert_hf_to_gguf.py /full_model --outfile /quantized_tomato.gguf --outtype f32\n",
    "\n",
    "# 양자화 적용\n",
    "!./build/bin/llama-quantize /quantized_tomato.gguf /quantized_tomatoQ4_K_M.gguf Q4_K_M"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
