{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402c26c3",
   "metadata": {},
   "source": [
    "lora 어댑터와 기존 프루닝된 모델 업로드 후 결합하여 저장 - 재윤 로라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a79c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from model_loader import load_pruned_model\n",
    "\n",
    "model_path = \"/pruned_activation\"\n",
    "base_model, tokenizer = load_pruned_model(\n",
    "    model_path,\n",
    "    device=torch.device(\"cuda\")\n",
    ")\n",
    "full_model = PeftModel.from_pretrained(base_model, \"/lora_tomato\")\n",
    "\n",
    "full_model = full_model.merge_and_unload()\n",
    "\n",
    "# full model 저장\n",
    "full_model.save_pretrained(\"/full_model\")\n",
    "tokenizer.save_pretrained(\"/full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033f28c",
   "metadata": {},
   "source": [
    "lora 어댑터와 기존 프루닝된 모델 업로드 후 결합하여 저장 - 성민 로라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63366447",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from safetensors.torch import load_file\n",
    "from peft import PeftModel\n",
    "import torch.nn as nn\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/smartfarm_pruning/models/pruned_activation\", use_fast=False, local_files_only=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "added_token_path = \"/content/drive/MyDrive/smartfarm_pruning/models/pruned_activation/added_tokens.json\"\n",
    "\n",
    "with open(added_token_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            n_added = tokenizer.add_tokens(list(set(data)))\n",
    "            print(f\"Added {n_added} tokens from added_tokens.json\")\n",
    "\n",
    "pruned_path     = \"/content/drive/MyDrive/smartfarm_pruning/models/pruned_activation/pruned_structure.json\"\n",
    "state_dict_path = \"/content/drive/MyDrive/smartfarm_pruning/models/pruned_activation/model.safetensors\"\n",
    "\n",
    "with open(pruned_path, \"r\") as f:\n",
    "    pruned_info = json.load(f)\n",
    "\n",
    "if \"layer_structure\" not in pruned_info:\n",
    "    raise ValueError(\"❌ 'layer_structure' not found in pruned_structure.json\")\n",
    "\n",
    "layer_sizes = {int(k): v[\"intermediate_size\"] for k, v in pruned_info[\"layer_structure\"].items()}\n",
    "print(f\"Detected {len(layer_sizes)} layers from pruning info.\")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"/content/drive/MyDrive/smartfarm_pruning/models/pruned_activation\", trust_remote_code=False, local_files_only=True)\n",
    "\n",
    "# 베이스 모델(구조) 만들기\n",
    "base_model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "for i, layer in enumerate(base_model.model.layers):\n",
    "    if i in layer_sizes:\n",
    "        new_dim = layer_sizes[i]\n",
    "        in_dim  = layer.mlp.up_proj.weight.shape[1]\n",
    "        # gate/up/down 교체\n",
    "        layer.mlp.gate_proj = nn.Linear(in_dim, new_dim, bias=False)\n",
    "        layer.mlp.up_proj   = nn.Linear(in_dim, new_dim, bias=False)\n",
    "        layer.mlp.down_proj = nn.Linear(new_dim, in_dim, bias=False)\n",
    "\n",
    "print(\"Structure rebuilt. Loading pruned weights...\")\n",
    "state_dict = load_file(state_dict_path)\n",
    "missing, unexpected = base_model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"Weights loaded (missing={len(missing)}, unexpected={len(unexpected)})\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 토크나이저 토큰 수 반영\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.to(device)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, \"/content/drive/MyDrive/smartfarm_pruning/lora_tomato/LoRA_adapter\")\n",
    "\n",
    "# LoRA 병합\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 병합 모델 저장\n",
    "merged_model.save_pretrained(\"/content/drive/MyDrive/full_model\", safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59955c80",
   "metadata": {},
   "source": [
    "gguf 파일 생성 및 4bit 양자화 적용 (로컬에서는 아래 명령어 느낌표 제거 후 터미널에서 순차적으로 진행하면 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df43a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# gguf 파일 생성\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "%cd /content/llama.cpp\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "!mkdir -p build\n",
    "!cmake -B build\n",
    "!cmake --build build\n",
    "\n",
    "!python convert_hf_to_gguf.py /full_model --outfile /quantized_tomato.gguf --outtype f32\n",
    "\n",
    "# 양자화 적용\n",
    "!./build/bin/llama-quantize /quantized_tomato.gguf /quantized_tomatoQ4_K_M.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479bd61",
   "metadata": {},
   "source": [
    "모델 결합 및 양자화 gguf 변환 코드 모듈화 (함수 그대로 써서 파일 이름만 파라미터로 넣으면 됨. 여러개 한번에 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4895d1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ggerganov/llama.cpp.git\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "!mkdir -p build\n",
    "!cmake -B build\n",
    "!cmake --build build\n",
    "\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "def export_pruned_model_to_gguf(name: str):\n",
    "    pruned_path = f\"/content/drive/MyDrive/smartfarm_pruning/1st/models/pruned/activation/{name}\"\n",
    "    lora_path   = f\"/content/drive/MyDrive/smartfarm_pruning/1st/models/LoRA/activation/{name}\"\n",
    "    out_hf_path = f\"/content/full_model_{name}\"\n",
    "    out_gguf    = f\"/content/drive/MyDrive/{name}.gguf\"\n",
    "\n",
    "    # 프루닝 모델 로드\n",
    "    base_model, tokenizer = load_pruned_model(\n",
    "        pruned_path,\n",
    "        device=\"cuda\",\n",
    "        dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # LoRA 병합\n",
    "    full_model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "    full_model = full_model.merge_and_unload()\n",
    "\n",
    "    # 병합된 모델 저장\n",
    "    full_model.save_pretrained(out_hf_path)\n",
    "    tokenizer.save_pretrained(out_hf_path)\n",
    "\n",
    "    # GGUF 변환\n",
    "    os.system(f\"\"\"\n",
    "    cd /content/llama.cpp && \\\n",
    "    python convert_hf_to_gguf.py {out_hf_path} \\\n",
    "        --outfile {out_gguf} \\\n",
    "        --outtype q8_0\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"\\n양자화 완료: {out_gguf}\")\n",
    "\n",
    "export_pruned_model_to_gguf(\"pruned_activation_5\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
