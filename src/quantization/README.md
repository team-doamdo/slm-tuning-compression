## 전체 흐름

이 코드는 **프루닝된(Pruned) 모델**과 **LoRA 어댑터**를 결합하여 완전한(full) 모델을 생성하고,  
이를 **GGUF 포맷으로 변환 및 4bit 양자화**하는 과정을 수행한다.

전체 과정의 흐름은 다음과 같다.

1. **저장된 LoRA 어댑터 및 프루닝된 모델 업로드**
2. **두 모델을 결합하여 Full 모델 저장**
3. **저장된 Full 모델을 GGUF 파일로 변환**
4. **GGUF 파일에 4bit 양자화 적용**

> 모든 실험은 **Google Colab** 환경에서 진행되었다.

## LoRA 어댑터와 프루닝된 모델의 결합

- LoRA 어댑터 디렉토리에는 `config.json` 파일이 존재하지 않는다.
  따라서 모델 구조 정보를 가진 프루닝된 모델과 결합 과정이 필요하다.  
- 이후 두 모델을 결합한 결과물인 full 모델에는  
  `config.json`이 포함되어 있는 것을 확인할 수 있다.

## GGUF 변환 및 4bit 양자화

- GGUF 변환 시점에서는 최대 16bit까지만 변환이 가능하므로, 우선 비양자화(f32) 형태의 GGUF 파일을 생성한다. 이후 별도의 단계에서 `llama-quantize` 도구를 사용해 4bit 양자화(Q4_K_M)를 적용한다.

- 이 과정을 통해 생성된 두 GGUF 파일의 크기는 다음과 같다.

| 파일명 | 설명 | 크기 |
|:--|:--|--:|
| `quantized_tomato.gguf` | gguf 변환만 수행 | 약 **3 GB** |
| `quantized_tomatoQ4_K_M.gguf` | 4bit 양자화 적용 | 약 **800 MB** |

> 양자화 적용 후 모델 크기가 **약 75% 이상 감소**하였으며, 추론 시 메모리 사용량과 로딩 속도 면에서 효율이 크게 향상되었다.