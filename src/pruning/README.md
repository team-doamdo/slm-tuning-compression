# SmartFarm Pruning 

---

## 프루닝 방법론

### Activation 기반 중요도 측정

실제 스마트팜 데이터로 모델을 실행하여 각 FFN 뉴런의 활성화 패턴을 측정

```python
importance_score = (
    0.7 * mean_activation_normalized +
    0.2 * max_activation_normalized +
    0.1 * activity_ratio
) * layer_importance_scale
```

### 프루닝 전략

- **전역 프루닝**: 전체 뉴런 중 중요도 하위 N% 제거
- **레이어별 제약**: 각 레이어 최대 제거 비율 제한
- **최소 뉴런 보장**: 레이어당 최소 유지 뉴런 수 설정
- **레이어 차별화**: 초기/중간/후기 레이어에 다른 중요도 가중치 적용

### 주요 하이퍼파라미터

| 파라미터 | 설명 | 탐색 범위 |
| --- | --- | --- |
| `PRUNE_RATIO` | 전체 제거 비율 | 0.05 ~ 0.20 |
| `MAX_PRUNE_PER_LAYER` | 레이어당 최대 제거 비율 | 0.20 ~ 0.40 |
| `MIN_NEURONS_PER_LAYER` | 레이어당 최소 유지 뉴런 | 500 ~ 1000 |
| `LAYER_IMPORTANCE_SCALE` | 레이어별 중요도 가중치 | early: 0.3~0.8, middle: 1.0, late: 1.2~2.0 |
| `IMPORTANCE_WEIGHTS` | 중요도 계산 가중치 | mean/max/activity 조합 |

---

## 실험 설계

5단계 Phase로 최적 하이퍼파라미터 탐색:

### Phase 0: 베이스라인

- **실험 0**: 원본 모델 성능 측정 (프루닝 없음)

### Phase 1: 최소 프루닝 (안정성 확인)

- **실험 1-1**: 5% 프루닝

### Phase 2: 점진적 프루닝 강화

- **실험 2-1**: 10% 프루닝
- **실험 2-2**: 15% 프루닝
- **실험 2-3**: 20% 프루닝
- **실험 2-4**: 25% 프루닝
- **실험 2-5**: 30% 프루닝

### Phase 3: 레이어 스케일 최적화

- **실험 3-1**: 초기 레이어 공격적 프루닝
- **실험 3-2**: 중간 레이어 보호

### Phase 4: 중요도 가중치 튜닝

- **실험 4-1**: Dead neuron 제거 중심
- **실험 4-2**: 피크 활성화 중심
- **실험 4-3**: 균형 활성화 중심

### Phase 5: 최종 최적화

- 이전 단계 결과 종합하여 최적 조합 선정

---

## 평가 지표

| 지표 | 설명 | 목표 |
| --- | --- | --- |
| **ROUGE-L** | 생성 텍스트 품질 (recall 중심) | 원본 대비 95%+ 유지 |
| **BLEU** | 생성 텍스트 정확도 | 원본 대비 90%+ 유지 |
| **평균 응답 시간** | 추론 속도 (초) | 30%+ 개선 |
| **메모리 사용량** | GPU 메모리 (MB) | 20%+ 감소 |
| **파라미터 수** | 모델 크기 | 프루닝 비율에 비례 감소 |

---

## 실험 결과

> 실험 환경: Google Colab (T4 GPU, 고용량 RAM) - 빠른 결과 확인을 위해 임시 사용
> 

### 전체 요약

| ID | Prune % | Parameters | Memory | ROUGE-L | BLEU | Response Time | 비고 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| origin | 0% | 999885952 | 3826.263671875 | 0.04866062079227732 | 0.0011557676927867603 | 2.541024950274139 | 원본 모델 |
| 1-1 | 5% | 972566272 | 3738.71142578125 | 0.047481628904715065 | 0.0 | 2.6822182389237974 | Phase1 최적 |
| 2-1 | 10% | 944945920 | 3641.138671875 | 0.043273461845167505 | 0.0 | 2.6139827787026366 |  |
| 2-2 | 15% | 915082624 | 3526.59912109375 | 0.0472001698446162 | 0.005749305310508111 | 2.573580858663726 |  |
| 2-3 | 20% | 885222784 | 3408.19677734375 | 0.04466729390615288 | 0.006688948238490611 | 2.443074292417942 |  |
| 2-4 | 25% | 855362944 | 3288.07568359375 | 0.06566234618786866 | 0.03392401004940847 | 2.4523856567025426 | Phase2 최적 |
| 2-5 | 30% | 825506560 | 3192.4697265625 | 0.03613416588006313 | 0.008972149683440088 | 2.459288539808782 |  |
| 3-1 | 25% | 855362944 | 3288.07568359375 | 0.06573085067773667 | 0.03392401004940847 | 2.4374868833363177 | Phase3 최적 |
| 3-2 | 25% | 855362944 | 3288.07568359375 | 0.06569755084816578 | 0.03392401004940847 | 2.4653706800670583 |  |
| 4-1 | 25% | 855362944 | 3281.388671875 | 0.06952527321802858 | 0.04091745046569054 | 2.485657757751325 |  |
| 4-2 | 25% | 855362944 | 3281.388671875 | 0.041422031158746916 | 0.011762364037628838 | 2.4914752803363527 |  |
| 4-3 | 25% | 855362944 | 3285.8779296875 | 0.07215828875383 | 0.043472447700814064 | 2.4616527732180966 | Phase4 최적 |

### 최종 모델 선정: 실험 4-3

### 선정 이유

- **ROUGE-L 148% 향상** (0.0487 → 0.0722)
- **BLEU 3,661% 향상** (0.0012 → 0.0435)
- **추론 속도 3.1% 개선** (2.541s → 2.462s)
- **메모리 14% 감소** (3826MB → 3286MB)
- **파라미터 14% 감소** (1.00B → 0.86B)

### 최적 설정

```python
PRUNE_RATIO = 0.25
MAX_PRUNE_PER_LAYER = 0.45
MIN_NEURONS_PER_LAYER = 400
LAYER_IMPORTANCE_SCALE = {'early': 0.2, 'middle': 1.0, 'late': 2.2}
IMPORTANCE_WEIGHTS = {'mean_activation': 0.8, 'max_activation': 0.1, 'activity_ratio': 0.1}
```
