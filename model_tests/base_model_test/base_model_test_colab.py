from huggingface_hub import login
import psutil, os


# ğŸ¤« Hugging Face ì›¹ì—ì„œ ë°œê¸‰í•œ í† í° ë„£ê¸°
hf_token = ""

# ë¡œê·¸ì¸
login(token=hf_token)
print("âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ")
# Google Colabìš© ì–¸ì–´ ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

import time
import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from google.colab import files
import os

# --------------------------
# ì„¤ì •
# --------------------------

# ì½”ë© í™˜ê²½ì— ë§ê²Œ ê°€ë²¼ìš´ ëª¨ë¸ë“¤ë¡œ ì„ ë³„
models = [
    # "google/gemma-2-2b-it",
    # "meta-llama/Llama-3.2-1B-Instruct",
    # "meta-llama/Llama-3.2-3B-Instruct",  # ë” ì•ˆì •ì ì¸ TinyLlama ëª¨ë¸
    # "google/gemma-3-1b-it",  # ê°€ë²¼ìš´ ëŒ€í™”í˜• ëª¨ë¸
    # "microsoft/Phi-4-mini-instruct",
    # "Qwen/Qwen2.5-1.5B-Instruct", #ë² ì´ìŠ¤ ëª¨ë¸
    "deepseek-ai/deepseek-coder-1.3b-instruct"
]

# ì‹¤ì œ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
prompts = [
    "What is 28 + 93?",  # basic_arithmetic
    "Which country has the largest population?",  # factual_knowledge
    "If you put a plant in a cave with no sunlight at all, why would it not survive for long?",  # scientific_reasoning
    "Context: On January 10, 2025, the New York City Metropolitan Transportation Authority (MTA) announced that it will add 265 new zero-emission electric buses to city bus routes. Together with the 60 buses introduced last year and the 205 that will begin operating by the end of this year, a total of 530 electric buses will be running throughout New York City. These new 40-foot buses use a regenerative braking system that recovers up to 90% of energy during braking and can reduce annual carbon dioxide emissions by up to 90 tons per bus. MTA Chair Janno Lieber stated, \"With the introduction of these electric buses and infrastructure upgrades at bus depots across the city, New Yorkers will be able to breathe cleaner air.\" In addition, a new charging infrastructure will be installed at the Jamaica Bus Depot to lay the foundation for future zero-emission bus operations. Task: Summarize the passage above into one sentence in English.",  # text_comprehension
    "2, 4, 8, 16, ? What is the next number in the sequence?",  # pattern_recognition_1
    "If January 1 is Monday and January 8 is also Monday, what day of the week is January 15?",  # pattern_recognition_2
    "The following temperature sensor readings are given: [22, 23, 22, 45, 23, 22]. Which value seems to be an outlier, and why?",  # outlier_detection_1
    "Here are the test scores: [85, 88, 90, 300, 87, 89]. Which score is an outlier, and why?",  # outlier_detection_2
    "Please write a short poem.",  # text_generation
    "Rewrite this sentence so that a child can understand it: \"The Earth revolves around the Sun, causing the change of seasons.\""  # text_editing
]

output_file = "model_eval_results.txt"

# --------------------------
# í•¨ìˆ˜ë“¤
# --------------------------

def setup_gpu():
    """CPU ì „ìš© ì„¤ì •"""
    print("â„¹ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    return False


# def measure_memory():
#     """CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB ë‹¨ìœ„)"""
#     process = psutil.Process(os.getpid())
#     cpu_mem = process.memory_info().rss / (1024 ** 2)  # MB
#     return cpu_mem


def measure_memory():
    """ì „ì²´ ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (MB ë‹¨ìœ„)"""
    mem = psutil.virtual_memory()
    # mem.usedëŠ” í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬ (MB)
    return mem.used / (1024 ** 2)

def clear_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def load_model_safely(model_name):
    """ì•ˆì „í•˜ê²Œ ëª¨ë¸ ë¡œë“œ"""
    try:
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # pad_tokenì´ ì—†ëŠ” ê²½ìš° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # ëª¨ë¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ float16 ì‚¬ìš©)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )

        model.eval()
        if torch.cuda.is_available():
            model = model.cuda()

        return model, tokenizer

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {str(e)}")
        return None, None

def test_model(model_name, model, tokenizer, prompts, f):
    """ê°œë³„ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    header = f"\n{'='*50}\nëª¨ë¸: {model_name}\n{'='*50}\n\n"
    f.write(header)
    print(header)  # ì½˜ì†”ì— ë°”ë¡œ ì¶œë ¥

    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    try:
        gen_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device=-1,
            do_sample=True,
            temperature=0.7,
            max_new_tokens=20,
            pad_token_id=tokenizer.pad_token_id
        )
    except Exception as e:
        error_msg = f"âŒ íŒŒì´í”„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {str(e)}\n"
        f.write(error_msg)
        print(error_msg)
        return

    total_time = 0
    successful_tests = 0

    for i, prompt in enumerate(prompts, 1):
        section_header = f"\n--- í”„ë¡¬í”„íŠ¸ {i} ---\nì§ˆë¬¸: {prompt}\n"
        f.write(section_header)
        print(section_header)

        try:
            start_mem = measure_memory()
            start_time = time.time()

            # í…ìŠ¤íŠ¸ ìƒì„±
            output = gen_pipeline(prompt, max_new_tokens=150)

            end_time = time.time()
            end_mem = measure_memory()

            generated_text = output[0]["generated_text"]
            answer = generated_text[len(prompt):].strip()

            answer_text = f"ë‹µë³€:\n{answer}\nâ±ï¸ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {end_mem - start_mem:.2f} MB\n"
            f.write(answer_text)
            print(answer_text)

            total_time += (end_time - start_time)
            successful_tests += 1

        except Exception as e:
            error_msg = f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}\n"
            f.write(error_msg)
            print(error_msg)

        f.write(f"{'-'*30}\n")
        print(f"{'-'*30}\n")

    # ëª¨ë¸ ìš”ì•½
    summary_text = f"\nğŸ“Š {model_name} ìš”ì•½:\nì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(prompts)}\n"
    if successful_tests > 0:
        summary_text += f"í‰ê·  ì‘ë‹µì‹œê°„: {total_time/successful_tests:.2f}ì´ˆ\n"
    f.write(summary_text)
    print(summary_text)

    for i, prompt in enumerate(prompts, 1):
        f.write(f"\n--- í”„ë¡¬í”„íŠ¸ {i} ---\n")
        f.write(f"ì§ˆë¬¸: {prompt}\n\n")

        try:
            start_mem = measure_memory()
            start_time = time.time()

            # í…ìŠ¤íŠ¸ ìƒì„±
            output = gen_pipeline(prompt, max_new_tokens=150)

            end_time = time.time()
            end_mem = measure_memory()

            # ê²°ê³¼ ì²˜ë¦¬
            generated_text = output[0]["generated_text"]
            # ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ
            answer = generated_text[len(prompt):].strip()

            f.write(f"ë‹µë³€:\n{answer}\n\n")
            f.write(f"â±ï¸ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\n")
            f.write(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {end_mem - start_mem:.2f} MB\n")

            total_time += (end_time - start_time)
            successful_tests += 1
        except Exception as e:
            f.write(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}\n")

        f.write(f"\n{'-'*30}\n")

    # ëª¨ë¸ ìš”ì•½
    f.write(f"\nğŸ“Š {model_name} ìš”ì•½:\n")
    f.write(f"ì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(prompts)}\n")
    if successful_tests > 0:
        f.write(f"í‰ê·  ì‘ë‹µì‹œê°„: {total_time/successful_tests:.2f}ì´ˆ\n")
    f.write(f"\n")

# --------------------------
# ë©”ì¸ ì‹¤í–‰ë¶€
# --------------------------

def main():
    print("ğŸš€ ì–¸ì–´ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*50)

    # GPU ì„¤ì • í™•ì¸
    gpu_available = setup_gpu()

    # ê²°ê³¼ íŒŒì¼ ìƒì„±
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("ì–¸ì–´ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n")
        f.write("="*50 + "\n")
        f.write(f"ì‹¤í–‰ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"GPU ì‚¬ìš©: {'ì˜ˆ' if gpu_available else 'ì•„ë‹ˆì˜¤'}\n")
        f.write(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìˆ˜: {len(models)}\n")
        f.write(f"í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}\n\n")

        for model_idx, model_name in enumerate(models, 1):
            print(f"\n[{model_idx}/{len(models)}] í…ŒìŠ¤íŠ¸ ì¤‘: {model_name}")

            # ëª¨ë¸ ë¡œë“œ
            model, tokenizer = load_model_safely(model_name)

            if model is not None and tokenizer is not None:
                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                test_model(model_name, model, tokenizer, prompts, f)
                print(f"âœ… {model_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            else:
                f.write(f"\nâŒ {model_name}: ë¡œë”© ì‹¤íŒ¨ë¡œ ìŠ¤í‚µ\n\n")
                print(f"âŒ {model_name} ìŠ¤í‚µë¨")

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model, tokenizer
            clear_memory()

            print(f"ì§„í–‰ë¥ : {model_idx}/{len(models)} ({model_idx/len(models)*100:.1f}%)")

    print(f"\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ê²°ê³¼ íŒŒì¼: {output_file}")

    # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì½”ë©ì—ì„œ)
    try:
        files.download(output_file)
        print(f"ğŸ“¥ ê²°ê³¼ íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except:
        print(f"ğŸ“„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ íŒŒì¼ íƒ­ì—ì„œ '{output_file}'ì„ í™•ì¸í•˜ì„¸ìš”.")

# ì‹¤í–‰
if __name__ == "__main__":
    main()