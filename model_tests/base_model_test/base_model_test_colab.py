from huggingface_hub import login
import psutil, os
import time
import torch
import gc
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from google.colab import files

# Hugging Face ë¡œê·¸ì¸
hf_token = ""
login(token=hf_token)
print("âœ… Hugging Face ë¡œê·¸ì¸ ì™„ë£Œ")

# --------------------------
# ì„¤ì •
# --------------------------
models = [
    # "google/gemma-2-2b-it",
    "meta-llama/Llama-3.2-1B-Instruct",
    # "meta-llama/Llama-3.2-3B-Instruct",  # ë” ì•ˆì •ì ì¸ TinyLlama ëª¨ë¸
    # "google/gemma-3-1b-it",  # ê°€ë²¼ìš´ ëŒ€í™”í˜• ëª¨ë¸
    # "microsoft/Phi-4-mini-instruct",
    # "Qwen/Qwen2.5-1.5B-Instruct", #ë² ì´ìŠ¤ ëª¨ë¸
    # "deepseek-ai/deepseek-coder-1.3b-instruct"
]

prompts = [
    "What is 28 + 93?",
    "Which country has the largest population?",
    "If you put a plant in a cave with no sunlight at all, why would it not survive for long?",
    "Context: On January 10, 2025, the New York City Metropolitan Transportation Authority (MTA) announced that it will add 265 new zero-emission electric buses to city bus routes. Task: Summarize in one sentence.",
    "2, 4, 8, 16, ? What is the next number?",
    "If January 1 is Monday and January 8 is also Monday, what day is January 15?",
    "The following temperature sensor readings: [22, 23, 22, 45, 23, 22]. Which value is an outlier?",
    "Here are the test scores: [85, 88, 90, 300, 87, 89]. Which score is an outlier?",
    "Please write a short poem.",
    "Rewrite this sentence for a child: 'The Earth revolves around the Sun, causing seasons.'"
]

output_file = "model_eval_results.txt"

# --------------------------
# í•¨ìˆ˜
# --------------------------
def measure_memory():
    return psutil.virtual_memory().used / (1024 ** 2)

def clear_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def load_model_safely(model_name):
    try:
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        model.eval()
        if torch.cuda.is_available():
            model = model.cuda()
        return model, tokenizer
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ {model_name}: {str(e)}")
        return None, None

def test_model(model_name, model, tokenizer, prompts, f):
    f.write(f"\n{'='*50}\nëª¨ë¸: {model_name}\n{'='*50}\n\n")
    print(f"\n{'='*50}\nëª¨ë¸: {model_name}\n{'='*50}\n")

    gen_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=-1,
        do_sample=True,
        temperature=0.7,
        max_new_tokens=150,
        pad_token_id=tokenizer.pad_token_id
    )

    total_time = 0
    successful_tests = 0

    for i, prompt in enumerate(prompts, 1):
        f.write(f"\n--- í”„ë¡¬í”„íŠ¸ {i} ---\nì§ˆë¬¸: {prompt}\n")
        print(f"\n--- í”„ë¡¬í”„íŠ¸ {i} ---\nì§ˆë¬¸: {prompt}\n")

        try:
            start_mem = measure_memory()
            start_time = time.time()
            output = gen_pipeline(prompt, max_new_tokens=150)
            end_time = time.time()
            end_mem = measure_memory()

            generated_text = output[0]["generated_text"]
            answer = generated_text[len(prompt):].strip()

            f.write(f"ë‹µë³€:\n{answer}\nâ±ï¸ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {end_mem - start_mem:.2f} MB\n")
            print(f"ë‹µë³€:\n{answer}\nâ±ï¸ ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: {end_mem - start_mem:.2f} MB\n")

            total_time += (end_time - start_time)
            successful_tests += 1
        except Exception as e:
            f.write(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}\n")
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")

        f.write(f"{'-'*30}\n")
        print(f"{'-'*30}\n")

    summary = f"\nğŸ“Š {model_name} ìš”ì•½:\nì„±ê³µí•œ í…ŒìŠ¤íŠ¸: {successful_tests}/{len(prompts)}\n"
    if successful_tests > 0:
        summary += f"í‰ê·  ì‘ë‹µì‹œê°„: {total_time/successful_tests:.2f}ì´ˆ\n"
    f.write(summary)
    print(summary)

# --------------------------
# ë©”ì¸
# --------------------------
def main():
    print("ğŸš€ ì–¸ì–´ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘\n" + "="*50)
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("ì–¸ì–´ëª¨ë¸ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼\n" + "="*50 + "\n")
        f.write(f"ì‹¤í–‰ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"GPU ì‚¬ìš©: ì•„ë‹ˆì˜¤\n")
        f.write(f"í…ŒìŠ¤íŠ¸ ëª¨ë¸ ìˆ˜: {len(models)}\n")
        f.write(f"í”„ë¡¬í”„íŠ¸ ìˆ˜: {len(prompts)}\n\n")

        for model_idx, model_name in enumerate(models, 1):
            model, tokenizer = load_model_safely(model_name)
            if model and tokenizer:
                test_model(model_name, model, tokenizer, prompts, f)
                print(f"âœ… {model_name} í…ŒìŠ¤íŠ¸ ì™„ë£Œ\n")
            else:
                f.write(f"\nâŒ {model_name}: ë¡œë”© ì‹¤íŒ¨ë¡œ ìŠ¤í‚µ\n")
                print(f"âŒ {model_name} ìŠ¤í‚µë¨\n")
            del model, tokenizer
            clear_memory()

    try:
        files.download(output_file)
        print(f"ğŸ“¥ ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {output_file}")
    except:
        print(f"ğŸ“„ íŒŒì¼ í™•ì¸: {output_file}")

if __name__ == "__main__":
    main()