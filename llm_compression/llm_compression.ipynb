{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ap6wpzcy3ZS8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 175508,
     "status": "ok",
     "timestamp": 1759136149563,
     "user": {
      "displayName": "황태식",
      "userId": "01152860659426259375"
     },
     "user_tz": -540
    },
    "id": "JtP6OKEtdzpm",
    "outputId": "0788fc41-d331-4a0f-c403-0190abc9019b"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade bitsandbytes transformers peft accelerate wandb\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "login(token=hf_token)\n",
    "\n",
    "!wandb login $os.environ[\"WANDB_API_KEY\"]\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "import torch.nn.utils.prune as prune\n",
    "from datasets import Dataset, load_dataset\n",
    "import time\n",
    "\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "\n",
    "# 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 비구조적 프루닝 적용\n",
    "for layer in model.model.layers:\n",
    "    for name in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "        prune.l1_unstructured(getattr(layer.mlp, name), name=\"weight\", amount=0.1)\n",
    "\n",
    "for layer in model.model.layers:\n",
    "    for name in [\"gate_proj\", \"up_proj\", \"down_proj\"]:\n",
    "        prune.remove(getattr(layer.mlp, name), \"weight\")\n",
    "\n",
    "# LoRA 적용\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"down_proj\",\"up_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 데이터셋 로드\n",
    "file_path = \"/content/drive/MyDrive/tomato_qa_3000.txt\"\n",
    "dataset = load_dataset(\"json\", data_files=file_path)[\"train\"]\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 데이터셋 토큰화\n",
    "def preprocess_function(ex):\n",
    "    input = f\"question: {ex['instruction']} answer: \"\n",
    "    label = ex[\"output\"]\n",
    "\n",
    "    model_inputs = tokenizer(input, truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "    label_tokens = tokenizer(label, truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = label_tokens[\"input_ids\"]\n",
    "\n",
    "    model_inputs[\"labels\"] = [\n",
    "        (l if l != tokenizer.pad_token_id else -100) for l in model_inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "# 학습 (Trainer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 최종 저장\n",
    "model.save_pretrained(\"/content/drive/MyDrive/results\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/results\")\n",
    "\n",
    "# 기존 gemma 모델 로드\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-3-1b-pt\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-3-1b-pt\")\n",
    "\n",
    "# 경량화된 모델 로드\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/content/drive/MyDrive/results\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/results\")\n",
    "\n",
    "text = \"question: How should nitrogen be managed after flowering begins? answer: \"\n",
    "\n",
    "# 기존 모델 시간\n",
    "inputs = base_tokenizer(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = base_model.generate(**inputs, max_new_tokens=128)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start}초 걸림\\n\")\n",
    "print(base_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# 경량화된 모델 시간\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"{end - start}초 걸림\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOtUzsrIsR1cpTFv7tT6FsD",
   "gpuType": "T4",
   "mount_file_id": "1LAj8v3DxNkCM0biRSD5vGABIHArLZbYq",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
