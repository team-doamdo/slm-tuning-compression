# -*- coding: utf-8 -*-
"""sLM_LoRA_Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e65NwsHvqVhQhMOyyuDrdy1Jj00JoDBF

í™˜ê²½ ì„¸íŒ…
"""

!pip install -U transformers accelerate trl peft datasets safetensors --no-cache-dir

"""êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ë° ëª¨ë¸ ë³µì‚¬"""

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/drive/MyDrive/pruned-model /content/
!ls -lh /content/pruned-model

"""ëª¨ë¸ ì¤€ë¹„"""

import os, torch, json, re
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- ê²½ë¡œ ì„¤ì • ---
MODEL_DIR = "/content/pruned-model"                # í”„ë£¨ë‹ëœ Gemma3-1B-pt ëª¨ë¸ í´ë”
DATA_FILE = "/content/finetuning_lora_data3.jsonl"  # LoRA í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ

OUT_DIR    = "/content/lora-out"       # í•™ìŠµ ì¶œë ¥ ê²½ë¡œ
MERGED_DIR = "/content/lora-merged"    # ë³‘í•© ëª¨ë¸ ì €ìž¥ ê²½ë¡œ

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(MERGED_DIR, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Environment ready. Using device: {device}")

"""í† í¬ë‚˜ì´ì € ë¡œë“œ"""

from transformers import AutoTokenizer
import json, os

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    print(f"â„¹ pad_token was None â†’ set to eos_token ({tokenizer.eos_token})")

# ì„ íƒ: pruned-model/added_tokens.json ìžˆìœ¼ë©´ ë°˜ì˜
added_token_path = os.path.join(MODEL_DIR, "added_tokens.json")
if os.path.exists(added_token_path):
    try:
        with open(added_token_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            n_added = tokenizer.add_tokens(list(set(data)))
            print(f"Added {n_added} tokens from added_tokens.json")
        else:
            print("â„¹ added_tokens.json is not a list. Skipped.")
    except Exception as e:
        print("added_tokens.json parse failed:", e)
else:
    print("â„¹ No added_tokens.json found.")

print(f"Tokenizer ready â€” vocab size: {len(tokenizer)}")

"""í”„ë£¨ë‹ êµ¬ì¡° ë°˜ì˜ ë° ê°€ì¤‘ì¹˜ ë¡œë“œ"""

from transformers import AutoConfig
from safetensors.torch import load_file
import torch.nn as nn
import json, os

pruned_path     = os.path.join(MODEL_DIR, "pruned_structure.json")
state_dict_path = os.path.join(MODEL_DIR, "model.safetensors")

with open(pruned_path, "r") as f:
    pruned_info = json.load(f)

if "layer_structure" not in pruned_info:
    raise ValueError("âŒ 'layer_structure' not found in pruned_structure.json")

layer_sizes = {int(k): v["intermediate_size"] for k, v in pruned_info["layer_structure"].items()}
print(f"Detected {len(layer_sizes)} layers from pruning info.")

# config â†’ from_pretrained (trust_remote_code í•„ìš”ì‹œ True)
config = AutoConfig.from_pretrained(MODEL_DIR, trust_remote_code=True)

# ë² ì´ìŠ¤ ëª¨ë¸(êµ¬ì¡°) ë§Œë“¤ê¸°
base_model = AutoModelForCausalLM.from_config(config)
# ë ˆì´ì–´ë³„ MLP ë¦¬ì‚¬ì´ì¦ˆ
for i, layer in enumerate(base_model.model.layers):
    if i in layer_sizes:
        new_dim = layer_sizes[i]
        in_dim  = layer.mlp.up_proj.weight.shape[1]
        # gate/up/down êµì²´
        layer.mlp.gate_proj = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.up_proj   = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.down_proj = nn.Linear(new_dim, in_dim, bias=False)

print("Structure rebuilt. Loading pruned weights...")
state_dict = load_file(state_dict_path)
missing, unexpected = base_model.load_state_dict(state_dict, strict=False)
print(f"Weights loaded (missing={len(missing)}, unexpected={len(unexpected)})")

# í† í¬ë‚˜ì´ì € í† í° ìˆ˜ ë°˜ì˜
base_model.resize_token_embeddings(len(tokenizer))
base_model.to(device)

print("Model structure aligned with pruning and weights loaded.")

"""LoRA ì ìš©"""

import torch.nn as nn
import re

CANDIDATES = [
    "q_proj","k_proj","v_proj","o_proj",
    "gate_proj","up_proj","down_proj",
    "wi","wo","wq","wk","wv","out_proj",
    "fc_in","fc_out"
]

def infer_target_modules(model):
    present = set()
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            for cand in CANDIDATES:
                if re.search(rf"\b{re.escape(cand)}\b", name):
                    present.add(cand)
    present = [m for m in sorted(present) if m != "lm_head"]
    # ìµœì†Œ ì„¸íŠ¸ ë³´ìž¥
    if not present:
        present = ["q_proj","v_proj","o_proj"]
    return sorted(set(present))

target_modules = infer_target_modules(base_model)
print("LoRA target modules:", target_modules)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=target_modules
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()

"""ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""

from datasets import load_dataset

raw = load_dataset("json", data_files={"train": DATA_FILE})["train"]

def to_text(example):
    instr = example["instruction"].strip()
    out   = example["output"].strip()
    return {"text": f"### Instruction:\n{instr}\n\n### Response:\n{out}\n"}

train_dataset = raw.map(to_text, remove_columns=raw.column_names)
print("Dataset size:", len(train_dataset))
print(train_dataset[0]["text"][:250])

"""í•™ìŠµ ì„¤ì • ë° í•™ìŠµ"""

from trl import SFTTrainer, SFTConfig

sft_config = SFTConfig(
    output_dir=OUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1.5e-4,
    num_train_epochs=2,
    fp16=True,
    save_steps=200,
    logging_steps=25,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=train_dataset,
    formatting_func=lambda ex: ex["text"],
)

trainer.train()

"""LoRA ì–´ëŒ‘í„° ì €ìž¥"""

"""í•™ìŠµ í›„ LoRA ì–´ëŒ‘í„° ì €ìž¥"""

# LoRA ì–´ëŒ‘í„°(ê°€ì¤‘ì¹˜)ë§Œ ë³„ë„ë¡œ ì €ìž¥
LORA_ADAPTER_DIR = "/content/lora-adapter"
os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)

# í•™ìŠµëœ ëª¨ë¸ì—ì„œ LoRA ê°€ì¤‘ì¹˜ë§Œ ì¶”ì¶œí•˜ì—¬ ì €ìž¥
model.save_pretrained(LORA_ADAPTER_DIR, safe_serialization=True)
tokenizer.save_pretrained(LORA_ADAPTER_DIR)

print(f"LoRA adapter weights saved to: {LORA_ADAPTER_DIR}")

"""LoRA ë³‘í•© ë° ì €ìž¥"""

from peft import PeftModel

LAST_CKPT = "/content/lora-out/checkpoint-616"

# base_modelì€ í”„ë£¨ë‹ëœ Gemma3-1B ë¡œë“œëœ ìƒíƒœì—¬ì•¼ í•¨
model = PeftModel.from_pretrained(base_model, LAST_CKPT)

# LoRA ë³‘í•©
merged_model = model.merge_and_unload()

# ë³‘í•© ëª¨ë¸ ì €ìž¥
merged_model.save_pretrained("/content/lora-merged", safe_serialization=True)
tokenizer.save_pretrained("/content/lora-merged")

print(f"Fully merged model saved to /content/lora-merged (from {LAST_CKPT})")

"""í•˜ì´ë¸Œë¦¬ë“œ ê·œì¹™ ê¸°ë°˜ ì¶”ë¡  ëž˜í¼"""

from transformers import AutoConfig, AutoModelForCausalLM
from safetensors.torch import load_file
import torch.nn as nn
import torch, json, os

# --- í”„ë£¨ë‹ êµ¬ì¡° ë³µì› ---
pruned_path = os.path.join(MODEL_DIR, "pruned_structure.json")   # MODEL_DIRì—ì„œ ë¶ˆëŸ¬ì˜´
state_dict_path = os.path.join(MERGED_DIR, "model.safetensors")

with open(pruned_path, "r") as f:
    pruned_info = json.load(f)

if "layer_structure" not in pruned_info:
    raise ValueError("âŒ 'layer_structure' key not found in pruned_structure.json")

layer_sizes = {
    int(k): v["intermediate_size"]
    for k, v in pruned_info["layer_structure"].items()
}

config = AutoConfig.from_pretrained(MERGED_DIR, trust_remote_code=True)
merged_model = AutoModelForCausalLM.from_config(config)

# --- ê° ë ˆì´ì–´ë³„ë¡œ MLP í¬ê¸° ì¡°ì • ---
for i, layer in enumerate(merged_model.model.layers):
    if i in layer_sizes:
        new_dim = layer_sizes[i]
        in_dim = layer.mlp.up_proj.weight.shape[1]
        layer.mlp.gate_proj = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.up_proj = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.down_proj = nn.Linear(new_dim, in_dim, bias=False)
        print(f"Layer {i}: MLP resized to {new_dim}")

# --- ê°€ì¤‘ì¹˜ ë¡œë“œ ì „ ---
merged_model.resize_token_embeddings(len(tokenizer))

# --- ê°€ì¤‘ì¹˜ ë¡œë“œ ---
state_dict = load_file(state_dict_path)
missing, unexpected = merged_model.load_state_dict(state_dict, strict=False)
print(f"Loaded weights (missing={len(missing)}, unexpected={len(unexpected)})")

device = "cuda" if torch.cuda.is_available() else "cpu"
merged_model.to(device)
merged_model.eval()

print("Pruned structure successfully restored for inference.")

"""ê·œì¹™ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ì¶”ë¡  í•¨ìˆ˜"""

def hybrid_infer(user_question, temp=None, hum=None, co2=None, light=None, max_new_tokens=128):
    RANGES = {"temp": (20,25), "hum": (65,75), "co2": (800,1000), "light": (45000,70000)}
    alerts = []
    if temp is not None:
        lo, hi = RANGES["temp"]
        if temp > hi: alerts.append(f"TEMP_HIGH({int(temp)})")
        elif temp < lo: alerts.append(f"TEMP_LOW({int(temp)})")
    if hum is not None:
        lo, hi = RANGES["hum"]
        if hum > hi: alerts.append(f"HUM_HIGH({int(hum)})")
        elif hum < lo: alerts.append(f"HUM_LOW({int(hum)})")
    if co2 is not None:
        lo, hi = RANGES["co2"]
        if co2 > hi: alerts.append(f"CO2_HIGH({int(co2)})")
        elif co2 < lo: alerts.append(f"CO2_LOW({int(co2)})")
    if light is not None:
        lo, hi = RANGES["light"]
        if light < lo: alerts.append(f"LIGHT_LOW({int(light)})")

    prefix = f"[STATUS: {alerts[0]}]" if alerts else "[STATUS: OK]"
    prompt = f"### Instruction:\n{prefix} {user_question}\n\n### Response:\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = merged_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.25,
            top_p=0.9,
            repetition_penalty=1.05,
            pad_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = decoded.split("### Response:")[-1].strip()

    print(f"\n[STATUS PROMPT] {prefix}")
    print("ðŸ§© Question:", user_question)
    print("ðŸ’¬ Model Response:\n", response)

"""ì¶”ë¡  í…ŒìŠ¤íŠ¸"""

print("\n[ì •ìƒ ìƒíƒœ ì˜ˆì‹œ]")
hybrid_infer("Is the condition good?", temp=23, hum=70, co2=900, light=50000)

print("\n[ì˜¨ë„ ë†’ì€ ìƒí™© ì˜ˆì‹œ]")
hybrid_infer("light is good?", temp=55, hum=70, co2=900, light=50000)

print("\n[ìŠµë„ ë‚®ì€ ìƒí™© ì˜ˆì‹œ]")
hybrid_infer("is the enviroment okay?", temp=22, hum=50, co2=850, light=48000)