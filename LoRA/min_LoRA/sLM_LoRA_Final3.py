# -*- coding: utf-8 -*-
"""sLM_LoRA_Final

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e65NwsHvqVhQhMOyyuDrdy1Jj00JoDBF

환경 세팅
"""

!pip install -U transformers accelerate trl peft datasets safetensors --no-cache-dir

"""구글 드라이브 연결 및 모델 복사"""

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/drive/MyDrive/pruned-model /content/
!ls -lh /content/pruned-model

"""모델 준비"""

import os, torch, json, re
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- 경로 설정 ---
MODEL_DIR = "/content/pruned-model"                # 프루닝된 Gemma3-1B-pt 모델 폴더
DATA_FILE = "/content/finetuning_lora_data3.jsonl"  # LoRA 학습 데이터셋 경로

OUT_DIR    = "/content/lora-out"       # 학습 출력 경로
MERGED_DIR = "/content/lora-merged"    # 병합 모델 저장 경로

os.makedirs(OUT_DIR, exist_ok=True)
os.makedirs(MERGED_DIR, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Environment ready. Using device: {device}")

"""토크나이저 로드"""

from transformers import AutoTokenizer
import json, os

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=False)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    print(f"ℹ pad_token was None → set to eos_token ({tokenizer.eos_token})")

# 선택: pruned-model/added_tokens.json 있으면 반영
added_token_path = os.path.join(MODEL_DIR, "added_tokens.json")
if os.path.exists(added_token_path):
    try:
        with open(added_token_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, list):
            n_added = tokenizer.add_tokens(list(set(data)))
            print(f"Added {n_added} tokens from added_tokens.json")
        else:
            print("ℹ added_tokens.json is not a list. Skipped.")
    except Exception as e:
        print("added_tokens.json parse failed:", e)
else:
    print("ℹ No added_tokens.json found.")

print(f"Tokenizer ready — vocab size: {len(tokenizer)}")

"""프루닝 구조 반영 및 가중치 로드"""

from transformers import AutoConfig
from safetensors.torch import load_file
import torch.nn as nn
import json, os

pruned_path     = os.path.join(MODEL_DIR, "pruned_structure.json")
state_dict_path = os.path.join(MODEL_DIR, "model.safetensors")

with open(pruned_path, "r") as f:
    pruned_info = json.load(f)

if "layer_structure" not in pruned_info:
    raise ValueError("❌ 'layer_structure' not found in pruned_structure.json")

layer_sizes = {int(k): v["intermediate_size"] for k, v in pruned_info["layer_structure"].items()}
print(f"Detected {len(layer_sizes)} layers from pruning info.")

# config → from_pretrained (trust_remote_code 필요시 True)
config = AutoConfig.from_pretrained(MODEL_DIR, trust_remote_code=True)

# 베이스 모델(구조) 만들기
base_model = AutoModelForCausalLM.from_config(config)
# 레이어별 MLP 리사이즈
for i, layer in enumerate(base_model.model.layers):
    if i in layer_sizes:
        new_dim = layer_sizes[i]
        in_dim  = layer.mlp.up_proj.weight.shape[1]
        # gate/up/down 교체
        layer.mlp.gate_proj = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.up_proj   = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.down_proj = nn.Linear(new_dim, in_dim, bias=False)

print("Structure rebuilt. Loading pruned weights...")
state_dict = load_file(state_dict_path)
missing, unexpected = base_model.load_state_dict(state_dict, strict=False)
print(f"Weights loaded (missing={len(missing)}, unexpected={len(unexpected)})")

# 토크나이저 토큰 수 반영
base_model.resize_token_embeddings(len(tokenizer))
base_model.to(device)

print("Model structure aligned with pruning and weights loaded.")

"""LoRA 적용"""

import torch.nn as nn
import re

CANDIDATES = [
    "q_proj","k_proj","v_proj","o_proj",
    "gate_proj","up_proj","down_proj",
    "wi","wo","wq","wk","wv","out_proj",
    "fc_in","fc_out"
]

def infer_target_modules(model):
    present = set()
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            for cand in CANDIDATES:
                if re.search(rf"\b{re.escape(cand)}\b", name):
                    present.add(cand)
    present = [m for m in sorted(present) if m != "lm_head"]
    # 최소 세트 보장
    if not present:
        present = ["q_proj","v_proj","o_proj"]
    return sorted(set(present))

target_modules = infer_target_modules(base_model)
print("LoRA target modules:", target_modules)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=target_modules
)

model = get_peft_model(base_model, lora_config)
model.print_trainable_parameters()

"""데이터 로드 및 전처리"""

from datasets import load_dataset

raw = load_dataset("json", data_files={"train": DATA_FILE})["train"]

def to_text(example):
    instr = example["instruction"].strip()
    out   = example["output"].strip()
    return {"text": f"### Instruction:\n{instr}\n\n### Response:\n{out}\n"}

train_dataset = raw.map(to_text, remove_columns=raw.column_names)
print("Dataset size:", len(train_dataset))
print(train_dataset[0]["text"][:250])

"""학습 설정 및 학습"""

from trl import SFTTrainer, SFTConfig

sft_config = SFTConfig(
    output_dir=OUT_DIR,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1.5e-4,
    num_train_epochs=2,
    fp16=True,
    save_steps=200,
    logging_steps=25,
    report_to="none",
)

trainer = SFTTrainer(
    model=model,
    args=sft_config,
    train_dataset=train_dataset,
    formatting_func=lambda ex: ex["text"],
)

trainer.train()

"""LoRA 어댑터 저장"""

"""학습 후 LoRA 어댑터 저장"""

# LoRA 어댑터(가중치)만 별도로 저장
LORA_ADAPTER_DIR = "/content/lora-adapter"
os.makedirs(LORA_ADAPTER_DIR, exist_ok=True)

# 학습된 모델에서 LoRA 가중치만 추출하여 저장
model.save_pretrained(LORA_ADAPTER_DIR, safe_serialization=True)
tokenizer.save_pretrained(LORA_ADAPTER_DIR)

print(f"LoRA adapter weights saved to: {LORA_ADAPTER_DIR}")

"""LoRA 병합 및 저장"""

from peft import PeftModel

LAST_CKPT = "/content/lora-out/checkpoint-616"

# base_model은 프루닝된 Gemma3-1B 로드된 상태여야 함
model = PeftModel.from_pretrained(base_model, LAST_CKPT)

# LoRA 병합
merged_model = model.merge_and_unload()

# 병합 모델 저장
merged_model.save_pretrained("/content/lora-merged", safe_serialization=True)
tokenizer.save_pretrained("/content/lora-merged")

print(f"Fully merged model saved to /content/lora-merged (from {LAST_CKPT})")

"""하이브리드 규칙 기반 추론 래퍼"""

from transformers import AutoConfig, AutoModelForCausalLM
from safetensors.torch import load_file
import torch.nn as nn
import torch, json, os

# --- 프루닝 구조 복원 ---
pruned_path = os.path.join(MODEL_DIR, "pruned_structure.json")   # MODEL_DIR에서 불러옴
state_dict_path = os.path.join(MERGED_DIR, "model.safetensors")

with open(pruned_path, "r") as f:
    pruned_info = json.load(f)

if "layer_structure" not in pruned_info:
    raise ValueError("❌ 'layer_structure' key not found in pruned_structure.json")

layer_sizes = {
    int(k): v["intermediate_size"]
    for k, v in pruned_info["layer_structure"].items()
}

config = AutoConfig.from_pretrained(MERGED_DIR, trust_remote_code=True)
merged_model = AutoModelForCausalLM.from_config(config)

# --- 각 레이어별로 MLP 크기 조정 ---
for i, layer in enumerate(merged_model.model.layers):
    if i in layer_sizes:
        new_dim = layer_sizes[i]
        in_dim = layer.mlp.up_proj.weight.shape[1]
        layer.mlp.gate_proj = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.up_proj = nn.Linear(in_dim, new_dim, bias=False)
        layer.mlp.down_proj = nn.Linear(new_dim, in_dim, bias=False)
        print(f"Layer {i}: MLP resized to {new_dim}")

# --- 가중치 로드 전 ---
merged_model.resize_token_embeddings(len(tokenizer))

# --- 가중치 로드 ---
state_dict = load_file(state_dict_path)
missing, unexpected = merged_model.load_state_dict(state_dict, strict=False)
print(f"Loaded weights (missing={len(missing)}, unexpected={len(unexpected)})")

device = "cuda" if torch.cuda.is_available() else "cpu"
merged_model.to(device)
merged_model.eval()

print("Pruned structure successfully restored for inference.")

"""규칙 기반 하이브리드 추론 함수"""

def hybrid_infer(user_question, temp=None, hum=None, co2=None, light=None, max_new_tokens=128):
    RANGES = {"temp": (20,25), "hum": (65,75), "co2": (800,1000), "light": (45000,70000)}
    alerts = []
    if temp is not None:
        lo, hi = RANGES["temp"]
        if temp > hi: alerts.append(f"TEMP_HIGH({int(temp)})")
        elif temp < lo: alerts.append(f"TEMP_LOW({int(temp)})")
    if hum is not None:
        lo, hi = RANGES["hum"]
        if hum > hi: alerts.append(f"HUM_HIGH({int(hum)})")
        elif hum < lo: alerts.append(f"HUM_LOW({int(hum)})")
    if co2 is not None:
        lo, hi = RANGES["co2"]
        if co2 > hi: alerts.append(f"CO2_HIGH({int(co2)})")
        elif co2 < lo: alerts.append(f"CO2_LOW({int(co2)})")
    if light is not None:
        lo, hi = RANGES["light"]
        if light < lo: alerts.append(f"LIGHT_LOW({int(light)})")

    prefix = f"[STATUS: {alerts[0]}]" if alerts else "[STATUS: OK]"
    prompt = f"### Instruction:\n{prefix} {user_question}\n\n### Response:\n"

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = merged_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.25,
            top_p=0.9,
            repetition_penalty=1.05,
            pad_token_id=tokenizer.eos_token_id,
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = decoded.split("### Response:")[-1].strip()

    print(f"\n[STATUS PROMPT] {prefix}")
    print("🧩 Question:", user_question)
    print("💬 Model Response:\n", response)

"""추론 테스트"""

print("\n[정상 상태 예시]")
hybrid_infer("Is the condition good?", temp=23, hum=70, co2=900, light=50000)

print("\n[온도 높은 상황 예시]")
hybrid_infer("light is good?", temp=55, hum=70, co2=900, light=50000)

print("\n[습도 낮은 상황 예시]")
hybrid_infer("is the enviroment okay?", temp=22, hum=50, co2=850, light=48000)