
---

## ğŸ““ ë…¸íŠ¸ë¶ ì„¤ëª…
### `fine_tuning_pruning.ipynb`
#### ì£¼ìš” ê¸°ëŠ¥
1. **ëª¨ë¸ ë¡œë“œ**
   - `google/gemma-3-1b-pt` ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸  
   - `transformers`, `torch` ì‚¬ìš©  

2. **Baseline ì¶”ë¡  (`quick_infer`)**
   - CPU í™˜ê²½ì—ì„œ í† í° ìƒì„± ì‹œê°„ ì¸¡ì •  
   - í”„ë£¨ë‹ ì „í›„ ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•œ baseline ì œê³µ  

3. **êµ¬ì¡°ì  í”„ë£¨ë‹ (`structured_prune_mlp`)**
   - ê° ë ˆì´ì–´ MLP projection weight norm ê¸°ë°˜ ì¤‘ìš”ë„ í‰ê°€  
   - Top-k ë°©ì‹ìœ¼ë¡œ ë‰´ëŸ° ì„ íƒ í›„ ì„ í˜• ê³„ì¸µ êµì²´  
   - keep ratio(ì˜ˆ: 0.7) ì¡°ì ˆ ê°€ëŠ¥  

4. **LoRA ê¸°ë°˜ íŒŒì¸íŠœë‹**
   - [PEFT](https://huggingface.co/docs/peft) í™œìš©  
   - `q_proj`, `k_proj`, `v_proj`, `o_proj`, `up_proj`, `down_proj`, `gate_proj` ëŒ€ìƒ ëª¨ë“ˆì— LoRA ì ìš©  
   - Hugging Face `Trainer` API + W&B ë¡œê¹… ì§€ì›  

5. **ë°ì´í„°ì…‹ ë¡œë“œ**
   - ì»¤ìŠ¤í…€ JSONL íŒŒì¼(`dataset.jsonl`) â†’ train/eval split (80:20)  
   - Instruction-following í˜•ì‹: `"Q: ...\nA: ..."`  

6. **í•™ìŠµ ê´€ë¦¬**
   - `TrainingArguments`ë¡œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ, gradient accumulation, learning rate, epochs, early stopping ë“± ì„¤ì •  
   - Best ëª¨ë¸ ì €ì¥ ë° `wandb` ì‹¤í—˜ ì¶”ì   

---

## ğŸ“‘ ë°ì´í„°ì…‹ ì„¤ëª…
### `dataset.jsonl`
- Instruction/Output ìŒìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì»¤ìŠ¤í…€ Tomato QA ë°ì´í„°ì…‹ (3000ê°œ ìƒ˜í”Œ)  
- ì˜ˆì‹œ:
```json
{"instruction": "What is the ideal daytime temperature for tomato plants?", "output": "The ideal daytime temperature is around 21-27Â°C (70-80Â°F)."}
