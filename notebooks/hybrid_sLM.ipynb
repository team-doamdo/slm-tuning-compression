{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDjo-HC36N_l"
   },
   "source": [
    "**프루닝 및 baseline 모델과의 속도차이 비교**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3f32595d2faf487898f7d88c368a92d4",
      "c32873d8f1f643f99ae4f379315fabc9",
      "bed10b47506c48febe383a3155d99d59",
      "05f1e91220db4a5e9bba63e4dbf2354a",
      "6651fb5f69e347cab4b9c9537ef74f89",
      "b9d6fe482b8941e0a187f412b4fda0a9",
      "3866b7241d1340d095e54b46da71b23d",
      "a98977ab865b4afc8e93818f78dd28a1",
      "8749624ac5084933a52108476f273191",
      "420fb4f715e247a3a5fd410ec937e36d",
      "0129c23f94d047fca5a59b18b2619d9a",
      "b62491f5921f4b968e9b0a8af1f6dede",
      "db88f6bdc48947738199096f35f88680",
      "b186bfb49a754c00a7bca05f98cd9c5a",
      "650945181d56491fa753f95750455e20",
      "2c6b7f5366504024a53275dc15010671",
      "7b9e853414c643d3a5baff0e84411000",
      "5ac4966e682049c9820cd0038078454e",
      "b04b3b50a2824aee80e60f6f4e1e84a8",
      "f434190b1954456482222db662f32e7a",
      "627bb40ac9f54bafbf2afc74e1c63eeb",
      "02f012a6512f418583c8964fc3b717a5",
      "b23f0df136394dc48ab03f4d5a45835f",
      "7bd008273674410e88736c2b409c68d3",
      "698c6889d9dc42239c893b40f8475040",
      "703b2399d10a4668a57c92e5120c48ce",
      "6aceb42efced4453b82c0832929056cc",
      "cacadc8135604a8883d73fc9840bdfd2",
      "7d7511497a8949cc9cd59e6b984c1dd4",
      "c9e4b45f0eed4f6f8b7f707bb72a397f",
      "aef8ba9a366d4d1da779bc63f60c740e",
      "439e450e368b4066b8df63488e730002",
      "565470263a5746d4ba03bf48f15e6d72",
      "0e9d95d7913b4aaabc107243f0276801",
      "d75806f0c54d42898db69094454b5e91",
      "2c91596f6a614e45be28a4e5f301dd5f",
      "968f9671ea824a1892796e715d41e520",
      "d5eeeddaab0a4c1f98fd995f9e250279",
      "c97aa5920ee748c49655370bb07d5c24",
      "fb06e4f22ed44280b59733992b677064",
      "5936edc109334cdda1c5a0ae7a79160e",
      "6628486b4d534d6395158cee144cc23e",
      "a8c2e05c5a914d11bb986a5b73c97cee",
      "566d73bf62784b0fa9052188bbdcbb35",
      "7421379f1ddd4e9f9d341b1f436b5d06",
      "f051807b2adb4ecb9239127560515b3b",
      "02af496d2cf74930a289edbbb1cd48bc",
      "5402b888db474f5fa667b4be76795805",
      "6d4dbdaaf963478d96f231fadccf8bd8",
      "575fbce5ec9b4f08a3f4c4961574fdeb",
      "cfef64482f42484d919030555a44eae6",
      "e976f35979f34002bb8e27d144a550bb",
      "4a51782e39de4b66bb8fbc23bac97c73",
      "4b95926e888d4e28bafe811c23f4a8af",
      "3b163efefb0341a9b7cbe3232925b77c",
      "19d18b01303349329c227c242fdca13a",
      "018405a1c12a4df89e737bf8be4fe608",
      "4270f41212cb40eab4af99a95eab20d3",
      "4bc2e7a044fd492b9131a9d0626b7e34",
      "c71958395ddc43efa9159a441ee860d2",
      "fdf40c3252a9487da0d278ed1e48dc73",
      "047f23a6f9a04e88b5be1ad87c4d76e7",
      "a9fc09435ec64cdaad94234d7649813f",
      "f39afbb81e6f43ad8f0dbf0004897570",
      "ca58d0761a3a46668c746640dc96df92",
      "0fda4716b7444b0a96b71472f8463fef",
      "49d27b69192f4d71ad9bb22b2ad1aeba",
      "9800d1d39d394929afa98da5c6dcb42a",
      "3ee26692d2fb4c5ba0b1d91c616d3f1e",
      "5cbe9e9314ba4c81af11916a0f1d0ff1",
      "bbfdbf0201f3423c90bb19d23f1547d5",
      "449c4eb03e7741f2957e5dd61fb4b4da",
      "2df6f02e8d2649e3aff8242cb75edb1e",
      "124e9c7f7b7647dda7e582ace9fc2767",
      "e9ddcad6876e49d1bac964294fc1c7f4",
      "93724cea75b64e0097fb3b0f335dd7af",
      "9dc84cd374a2430fb835e491fbe23b85",
      "89ef014bbb8242aebee95e3070e5a7b5",
      "a26d11407a1d483a9bb0e5abca9fee8c",
      "64802304b4fa45008015461d0808fc85",
      "b3fa00f4dc4a41259df3e696f1e67e36",
      "a41fb25b2e444e14a0181a729cd2e1a9",
      "78b9ae5a2cae43cdafcd7cd25c5f9932",
      "3fba27aaac1d48c095b63835b156c17a",
      "758f1a5e2aa04fb9a50bed448ce3c6db",
      "8a225ada3ff844d899e2c0a04ee410d9",
      "f1eef4cd34664655b612ac919a2e223c",
      "efe1fd11c3814954aa6bee4c8e186bea"
     ]
    },
    "id": "s6rf0x1Jyvk1",
    "outputId": "6b4ce648-f877-4cf7-e8ad-d16c3f578784"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch, time\n",
    "import torch.nn as nn\n",
    "\n",
    "# 필수 패키지 설치\n",
    "!pip install -q \"torch>=2.4.0\" \"transformers>=4.51.3\" accelerate sentencepiece\n",
    "\n",
    "# 허깅페이스 로그인\n",
    "!huggingface-cli login\n",
    "\n",
    "# 모델 로드\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "#레이어명 확인\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(name)\n",
    "\n",
    "model.eval()\n",
    "print(model.__class__.__name__, model.config.model_type, model.config.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-onleFd0BCsy"
   },
   "source": [
    "**추론 함수 및 프루닝 전 모델 추론 속도 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_A4e4ur3Ro9",
    "outputId": "99a83d73-dbf5-4c13-8a00-33f2435dcdfc"
   },
   "outputs": [],
   "source": [
    "def quick_infer(model, tokenizer, prompt=\"Hello\", max_new_tokens=80):\n",
    "    model_cpu = model.to(\"cpu\") #cpu로 런타임 유형 바꿔서 추론\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model_cpu.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.2, top_p=0.8)\n",
    "    elapsed = time.time() - start\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"\\n--- Prompt ---\\n{prompt}\\n\")\n",
    "    print(f\"--- Output ---\\n{text}\\n\")\n",
    "    print(f\"[Elapsed] {elapsed:.2f} sec for {max_new_tokens} tokens on CPU\\n\")\n",
    "    return elapsed\n",
    "\n",
    "# 예시 실행 (프루닝 전 baseline 측정)\n",
    "quick_infer(model, tokenizer, prompt=\"Is the temperature okay now?\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-mUzXvS5QVr"
   },
   "outputs": [],
   "source": [
    "def find_transformer_layers(model):\n",
    "    container = getattr(model, \"model\", None) or getattr(model, \"transformer\", None) or model\n",
    "    return getattr(container, \"layers\", None)\n",
    "\n",
    "def get_mlp(block):\n",
    "    mlp = block.mlp\n",
    "    return mlp.gate_proj, mlp.up_proj, mlp.down_proj\n",
    "\n",
    "@torch.no_grad()\n",
    "def structured_prune_mlp(model, keep_ratio=0.9):\n",
    "    layers = find_transformer_layers(model)\n",
    "    gate0, up0, down0 = get_mlp(layers[0])\n",
    "    inter = down0.in_features\n",
    "    new_inter = max(16, int(round(inter * keep_ratio)))\n",
    "    print(f\"[INFO] intermediate {inter} → {new_inter} (keep {keep_ratio*100:.0f}%)\")\n",
    "\n",
    "    for li, block in enumerate(layers):\n",
    "        gate, up, down = get_mlp(block)\n",
    "        score = torch.norm(up.weight, dim=1) + torch.norm(gate.weight, dim=1)\n",
    "        keep_idx = torch.topk(score, k=new_inter).indices.sort()[0]\n",
    "\n",
    "        def clone_linear(old, new_out, new_in, sel_out=None, sel_in=None):\n",
    "            new = nn.Linear(new_in, new_out, bias=old.bias is not None,\n",
    "                            dtype=old.weight.dtype, device=old.weight.device)\n",
    "            W = old.weight.data\n",
    "            if sel_out is not None: W = W[sel_out, :]\n",
    "            if sel_in is not None:  W = W[:, sel_in]\n",
    "            new.weight.data.copy_(W)\n",
    "            if old.bias is not None:\n",
    "                b = old.bias.data\n",
    "                if sel_out is not None: b = b[sel_out]\n",
    "                new.bias.data.copy_(b)\n",
    "            return new\n",
    "\n",
    "        new_gate = clone_linear(gate, new_inter, gate.in_features, sel_out=keep_idx)\n",
    "        new_up   = clone_linear(up,   new_inter, up.in_features,   sel_out=keep_idx)\n",
    "        new_down = clone_linear(down, down.out_features, new_inter, sel_in=keep_idx)\n",
    "\n",
    "        block.mlp.gate_proj, block.mlp.up_proj, block.mlp.down_proj = new_gate, new_up, new_down\n",
    "        if li % 4 == 0:\n",
    "            print(f\"  - layer {li}: kept {new_inter}/{inter}\")\n",
    "\n",
    "    model.config.intermediate_size = new_inter\n",
    "    print(\"[DONE] structured pruning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npSFb7QQ5skR",
    "outputId": "39b379a8-a78f-4c08-9422-51ca19a1ebc5"
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "structured_prune_mlp(model, keep_ratio=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCyWsTdkBJkA"
   },
   "source": [
    "**프루닝된 모델 추론 속도 테스트**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af4qHFsf5xeC",
    "outputId": "8a2aebbd-7c61-45b7-9be1-f8866e412700"
   },
   "outputs": [],
   "source": [
    "quick_infer(model, tokenizer, \"Is the temperature okay now?\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTeHup9Ofo7w"
   },
   "source": [
    "**규칙 기반 엔진**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM0pDrW4fokd"
   },
   "outputs": [],
   "source": [
    "def sensor_status(temp_c, hum, co2, lux):\n",
    "    # 기준\n",
    "    T_MIN, T_MAX = 20.0, 25.0\n",
    "    H_MIN, H_MAX = 65.0, 75.0\n",
    "    C_MIN, C_MAX = 800, 1000\n",
    "    L_MIN, L_MAX = 15000, 70000\n",
    "\n",
    "    def tri(v, lo, hi):\n",
    "        if v < lo: return \"LOW\"\n",
    "        if v > hi: return \"HIGH\"\n",
    "        return \"OK\"\n",
    "\n",
    "    return {\n",
    "        \"Temp\": tri(temp_c, T_MIN, T_MAX),\n",
    "        \"Humidity\": tri(hum, H_MIN, H_MAX),\n",
    "        \"CO2\": tri(co2, C_MIN, C_MAX),\n",
    "        \"Light\": tri(lux, L_MIN, L_MAX),\n",
    "    }\n",
    "\n",
    "def case_id_from_ok(status):\n",
    "    ok = {k: (v==\"OK\") for k,v in status.items()}\n",
    "    key = (\n",
    "        ok[\"Temp\"], ok[\"Humidity\"], ok[\"CO2\"],\n",
    "        ok[\"Light\"]\n",
    "    )\n",
    "    cid_map = {\n",
    "        (True, True, True, True): 1,\n",
    "        (True, True, True, False): 2,\n",
    "        (True, True, False, True): 3,\n",
    "        (True, True, False, False): 4,\n",
    "        (True, False, True, True): 5,\n",
    "        (True, False, True, False): 6,\n",
    "        (True, False, False, True): 7,\n",
    "        (True, False, False, False): 8,\n",
    "        (False, True, True, True): 9,\n",
    "        (False, True, True, False): 10,\n",
    "        (False, True, False, True): 11,\n",
    "        (False, True, False, False): 12,\n",
    "        (False, False, True, True): 13,\n",
    "        (False, False, True, False): 14,\n",
    "        (False, False, False, True): 15,\n",
    "        (False, False, False, False): 16,\n",
    "    }\n",
    "    return cid_map[key]\n",
    "\n",
    "def build_instruction(temp_c, hum_pct, co2_ppm, light_lux, user_question):\n",
    "    status = sensor_status(temp_c, hum_pct, co2_ppm, light_lux)\n",
    "    cid = case_id_from_ok(status)\n",
    "    status_line = f\"Status: Temp={status['Temp']}, Humidity={status['Humidity']}, CO2={status['CO2']}, Light={status['Light']}\"\n",
    "\n",
    "    summaries = {\n",
    "        1:  \"All parameters are within the optimal range.\",\n",
    "        2:  \"Only light is abnormal.\",\n",
    "        3:  \"Only CO2 is abnormal.\",\n",
    "        4:  \"CO2 and light are abnormal.\",\n",
    "        5:  \"Only humidity is abnormal.\",\n",
    "        6:  \"Humidity and light are abnormal.\",\n",
    "        7:  \"Humidity and CO2 are abnormal.\",\n",
    "        8:  \"Humidity, CO2, and light are abnormal.\",\n",
    "        9:  \"Only temperature is abnormal.\",\n",
    "        10: \"Temperature and light are abnormal.\",\n",
    "        11: \"Temperature and CO2 are abnormal.\",\n",
    "        12: \"Temperature, CO2, and light are abnormal.\",\n",
    "        13: \"Temperature and humidity are abnormal.\",\n",
    "        14: \"Temperature, humidity, and light are abnormal.\",\n",
    "        15: \"Temperature, humidity, and CO2 are abnormal.\",\n",
    "        16: \"All parameters are abnormal.\"\n",
    "    }\n",
    "\n",
    "    instruction = f\"Case #{cid}: {summaries[cid]}\\n{status_line}\\nQ: {user_question}\"\n",
    "\n",
    "    return instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29EZgOQ-6bBg"
   },
   "source": [
    "**프루닝된 모델을 LoRA 파인튜닝**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0QJ0k5s8eVC"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "fc5f64556c9a44d592e909183f0f513c",
      "2c056c6c59cd4a75a77e34baff8a5bd8",
      "4211181731ed4d6d9141c5c194ed0cc5",
      "d852f827c81144008c4bf0e2f9406155",
      "1ea8166248ae484dbce2199056b4183a",
      "c59126f3253d40acaa0c5dca163ce1e0",
      "ffae1f6ab55d4d75943b2699957f09de",
      "74070e13a0624881861718082a6a8a76",
      "ee70c602497d4442a52560719565cdd4",
      "d8a7c00dae264f9f9776d844786f8419",
      "5c6bfdf6bd66446f94861428a4969422",
      "b9768591f7e04510aa61a9f6d05ef728",
      "9bb1a47c78d24741b85c36ca12ad22cd",
      "6c6ad9944ec048559f9055a69b7cdda5",
      "ad8193be7d964ede876c404d983d0692",
      "62ec2f6abf74487e8e76d95b1ff1f296",
      "8567b4946e7d47f09135ca55294a813d",
      "6c910686eeb346ef9c5c590ad3f4e000",
      "5824d435078c46fd8351ce84f32d6ca4",
      "84d1f2055def47528d1dc00cbc5ced13",
      "b7d5d8bf07ac4650a1a5a4dad01f5d5f",
      "a2ec3f16fc144840bf429d1e253a61b9",
      "4fcd0c7335d1467fb46ceef174578332",
      "62e224f6664a467f8014c34e51d8cc70",
      "fcb24174fc92491980203b10d22723d3",
      "d78a5cc814804a6583c741d4b9b48376",
      "f2914b9d2d2d452fb9903f3d2adf5879",
      "0a063c859ae4408dadf394fef66f6dea",
      "35868b8616f5444a98fb8a7931fb9945",
      "8c45c9f5bc764dfd8de90a39791a56b0",
      "0cd10d3ce97b439e9c4d883f8455d782",
      "2b6e430b76cf43e98277420dfa432a29",
      "84a923a5188847689caf4631f79eb8d2"
     ]
    },
    "id": "5r9xw8WC87Sz",
    "outputId": "414ca4c5-869b-4c43-a0b9-9a682b309177"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# JSONL 파일 불러오기\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/smartfarm_qa_balanced_v5.jsonl\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)   # train:eval = 80:20\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset  = dataset[\"test\"]\n",
    "\n",
    "# 토크나이징 함수 (batched=True 지원)\n",
    "def tokenize_fn(examples):\n",
    "    texts = [f\"{ins}\\nA: {out} {tokenizer.eos_token}\"\n",
    "             for ins, out in zip(examples[\"instruction\"], examples[\"output\"])]\n",
    "    return tokenizer(texts, truncation=True, max_length=256, padding=\"max_length\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "eval_dataset  = eval_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "print(train_dataset[0])  # 토크나이즈 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "SAH1waFJ8irp",
    "outputId": "56042f73-e4ce-4512-daa0-d5a02aa7d0ab"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"tomato-qa\", name=\"tomato-qa-pruning-lora\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tomato\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=3000,\n",
    "    eval_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=3000,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"tomato-qa-pruning-lora\",\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"🔗 대시보드 URL:\", wandb.run.get_url())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0shu2WgV-W3B",
    "outputId": "482d7dc8-0250-42a8-dee7-c99a6b2cf8ae"
   },
   "outputs": [],
   "source": [
    "user_question = \"Is the enviroment okay now?\"\n",
    "prompt = build_instruction(\n",
    "    temp_c=24, hum_pct=75.0, co2_ppm=1900, light_lux=15000,\n",
    "    user_question=user_question\n",
    ")\n",
    "quick_infer(model, tokenizer, prompt, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LejQLqQaau-n"
   },
   "source": [
    "**GGUF 변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFPCxQRWayjC",
    "outputId": "dd82ed97-f48e-4ca4-9d2c-6d9571a67c10"
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import os, torch\n",
    "\n",
    "merged_out = \"/content/merged_gemma3_1b_pruned\"  # 프루닝+LoRA 병합 결과 경로\n",
    "os.makedirs(merged_out, exist_ok=True)\n",
    "\n",
    "# 현재 model이 PeftModel인지 확인\n",
    "print(type(model))\n",
    "\n",
    "# LoRA 병합\n",
    "model = model.merge_and_unload()  # PEFT 어댑터를 가중치에 통합\n",
    "model.save_pretrained(merged_out)\n",
    "tokenizer.save_pretrained(merged_out)\n",
    "\n",
    "print(\"LoRA 병합 완료:\", merged_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9KAEGvtgiQx",
    "outputId": "e1f3e0e3-09fd-4626-c380-9545dc87bf5f"
   },
   "outputs": [],
   "source": [
    "# 1. llama.cpp 설치\n",
    "!git clone --depth 1 https://github.com/ggerganov/llama.cpp\n",
    "%cd llama.cpp\n",
    "\n",
    "# 2. CMake로 빌드\n",
    "!cmake -B build\n",
    "!cmake --build build -j4\n",
    "\n",
    "# 3. 변환 스크립트에 필요한 라이브러리 설치\n",
    "!pip install mistral_common sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ckTTs6M5dcDw",
    "outputId": "fa959ce6-ee9d-48f7-a323-ada7b22a01c9"
   },
   "outputs": [],
   "source": [
    "# 4. Hugging Face 모델 -> GGUF 변환 (EOS/EOG 메타데이터 강제 지정)\n",
    "!python3 convert_hf_to_gguf.py \\\n",
    "  --outfile /content/gemma3_1b-pruned-f16.gguf \\\n",
    "  --outtype f16 \\\n",
    "  --metadata tokenizer.ggml.eos_token_id=1 \\\n",
    "  --metadata special_eog_token_ids=\"1,106\" \\\n",
    "  /content/merged_gemma3_1b_pruned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwd3vqHrmTcD"
   },
   "source": [
    "**4비트 양자화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPezPQiohpMu",
    "outputId": "8adfb2b1-cc78-42c2-9b3c-fde1763727cb"
   },
   "outputs": [],
   "source": [
    "# Q4_K_M 양자화 실행\n",
    "!/content/llama.cpp/build/bin/llama-quantize \\\n",
    "  /content/gemma3_1b-pruned-f16.gguf \\\n",
    "  /content/gemma3_1b-pruned-q4_k_m.gguf \\\n",
    "  q4_k_m\n",
    "\n",
    "# 산출물 확인\n",
    "!ls -lh /content/*.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ0VUFuE0Qn4"
   },
   "source": [
    "**Q5_K_M 양자화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YW_6R1MF0Tze",
    "outputId": "3ac4ec30-c224-422a-f11f-fabdafab44f8"
   },
   "outputs": [],
   "source": [
    "# Q5_K_M 양자화 실행\n",
    "!/content/llama.cpp/build/bin/llama-quantize \\\n",
    "  /content/gemma3_1b-pruned-f16.gguf \\\n",
    "  /content/gemma3_1b-pruned-q5_k_m.gguf \\\n",
    "  q5_k_m\n",
    "\n",
    "# 변환된 모델 파일 확인\n",
    "!ls -lh /content/*.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Btni3YqKQsqz"
   },
   "source": [
    "**래퍼 실햄 함수**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CT7qPeALQvd6"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_with_preprocessing(user_question,\n",
    "                           temp_c=22, hum_pct=10, co2_ppm=10, light_lux=10,\n",
    "                           model_path=\"/content/gemma3_1b-pruned-q5_k_m.gguf\"):\n",
    "\n",
    "    # 전처리\n",
    "    prompt = build_instruction(temp_c, hum_pct, co2_ppm, light_lux, user_question)\n",
    "    prompt = f\"{prompt}\\nA:\"\n",
    "\n",
    "    # llama.cpp 실행\n",
    "    result = subprocess.run([\n",
    "        \"./build/bin/llama-cli\",\n",
    "        \"-m\", model_path,\n",
    "        \"-p\", prompt,\n",
    "        \"-n\", \"256\",\n",
    "        \"--temp\", \"0.2\",\n",
    "        \"--top-p\", \"0.9\",\n",
    "        \"--top-k\", \"40\",\n",
    "        \"--repeat-penalty\", \"1.2\",\n",
    "        \"--threads\", \"4\",\n",
    "        \"-c\", \"512\"\n",
    "    ], capture_output=True, text=True)\n",
    "\n",
    "    raw_out = result.stdout\n",
    "    err_out = result.stderr\n",
    "\n",
    "    print(\"=== Raw Output ===\")\n",
    "    print(raw_out)\n",
    "\n",
    "    # A: 이후 부분만 추출\n",
    "    if \"A:\" in raw_out:\n",
    "        answer = raw_out.split(\"A:\", 1)[-1]\n",
    "    else:\n",
    "        answer = raw_out\n",
    "\n",
    "    # EOS 잘라내기\n",
    "    answer = answer.split(\"</s>\")[0].strip()\n",
    "\n",
    "    print(\"\\n=== Model Output ===\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WhapMSPQQ5H2",
    "outputId": "882309f5-8fa9-4f6b-f097-b1a42a183081"
   },
   "outputs": [],
   "source": [
    "# 모델 추론 테스트\n",
    "while True:\n",
    "    question = input(\"\\n❓ 질문을 입력하세요 (종료하려면 exit): \")\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "    run_with_preprocessing(question) # 라즈베리파이에서는 실시간 센싱값 받아서 매개변수에 넘겨줘야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q7iP_3RbhcC"
   },
   "source": [
    "**어댑터 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpwO7pxEbje1"
   },
   "outputs": [],
   "source": [
    "# LoRA 어댑터 저장\n",
    "model.save_pretrained(\"/content/lora_adapter\")\n",
    "tokenizer.save_pretrained(\"/content/lora_adapter\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
