{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDjo-HC36N_l"
   },
   "source": [
    "**í”„ë£¨ë‹ ë° baseline ëª¨ë¸ê³¼ì˜ ì†ë„ì°¨ì´ ë¹„êµ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6rf0x1Jyvk1",
    "outputId": "45aaa7f5-1ad2-438f-cdbb-ace80b069ce4"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch, time\n",
    "import torch.nn as nn\n",
    "\n",
    "# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q \"torch>=2.4.0\" \"transformers>=4.51.3\" accelerate sentencepiece\n",
    "\n",
    "# í—ˆê¹…í˜ì´ìŠ¤ ë¡œê·¸ì¸\n",
    "!huggingface-cli login\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "model_id = \"google/gemma-3-1b-pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "#ë ˆì´ì–´ëª… í™•ì¸\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        print(name)\n",
    "\n",
    "model.eval()\n",
    "print(model.__class__.__name__, model.config.model_type, model.config.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-onleFd0BCsy"
   },
   "source": [
    "**ì¶”ë¡  í•¨ìˆ˜ ë° í”„ë£¨ë‹ ì „ ëª¨ë¸ ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_A4e4ur3Ro9",
    "outputId": "8575a5b6-d998-4861-86cc-d3e4da926958"
   },
   "outputs": [],
   "source": [
    "def quick_infer(model, tokenizer, prompt=\"Hello\", max_new_tokens=30):\n",
    "    model_cpu = model.to(\"cpu\") #cpuë¡œ ëŸ°íƒ€ì„ ìœ í˜• ë°”ê¿”ì„œ ì¶”ë¡ \n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model_cpu.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.1, top_p=0.8)\n",
    "    elapsed = time.time() - start\n",
    "    text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(f\"\\n--- Prompt ---\\n{prompt}\\n\")\n",
    "    print(f\"--- Output ---\\n{text}\\n\")\n",
    "    print(f\"[Elapsed] {elapsed:.2f} sec for {max_new_tokens} tokens on CPU\\n\")\n",
    "    return elapsed\n",
    "\n",
    "# ì˜ˆì‹œ ì‹¤í–‰ (í”„ë£¨ë‹ ì „ baseline ì¸¡ì •)\n",
    "quick_infer(model, tokenizer, prompt=\"Q: What is the ideal daytime temperature for tomato plants?\\nA\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-mUzXvS5QVr"
   },
   "outputs": [],
   "source": [
    "def find_transformer_layers(model):\n",
    "    container = getattr(model, \"model\", None) or getattr(model, \"transformer\", None) or model\n",
    "    return getattr(container, \"layers\", None)\n",
    "\n",
    "def get_mlp(block):\n",
    "    mlp = block.mlp\n",
    "    return mlp.gate_proj, mlp.up_proj, mlp.down_proj\n",
    "\n",
    "@torch.no_grad()\n",
    "def structured_prune_mlp(model, keep_ratio=0.7):\n",
    "    layers = find_transformer_layers(model)\n",
    "    gate0, up0, down0 = get_mlp(layers[0])\n",
    "    inter = down0.in_features\n",
    "    new_inter = max(16, int(round(inter * keep_ratio)))\n",
    "    print(f\"[INFO] intermediate {inter} â†’ {new_inter} (keep {keep_ratio*100:.0f}%)\")\n",
    "\n",
    "    for li, block in enumerate(layers):\n",
    "        gate, up, down = get_mlp(block)\n",
    "        score = torch.norm(up.weight, dim=1) + torch.norm(gate.weight, dim=1)\n",
    "        keep_idx = torch.topk(score, k=new_inter).indices.sort()[0]\n",
    "\n",
    "        def clone_linear(old, new_out, new_in, sel_out=None, sel_in=None):\n",
    "            new = nn.Linear(new_in, new_out, bias=old.bias is not None,\n",
    "                            dtype=old.weight.dtype, device=old.weight.device)\n",
    "            W = old.weight.data\n",
    "            if sel_out is not None: W = W[sel_out, :]\n",
    "            if sel_in is not None:  W = W[:, sel_in]\n",
    "            new.weight.data.copy_(W)\n",
    "            if old.bias is not None:\n",
    "                b = old.bias.data\n",
    "                if sel_out is not None: b = b[sel_out]\n",
    "                new.bias.data.copy_(b)\n",
    "            return new\n",
    "\n",
    "        new_gate = clone_linear(gate, new_inter, gate.in_features, sel_out=keep_idx)\n",
    "        new_up   = clone_linear(up,   new_inter, up.in_features,   sel_out=keep_idx)\n",
    "        new_down = clone_linear(down, down.out_features, new_inter, sel_in=keep_idx)\n",
    "\n",
    "        block.mlp.gate_proj, block.mlp.up_proj, block.mlp.down_proj = new_gate, new_up, new_down\n",
    "        if li % 4 == 0:\n",
    "            print(f\"  - layer {li}: kept {new_inter}/{inter}\")\n",
    "\n",
    "    model.config.intermediate_size = new_inter\n",
    "    print(\"[DONE] structured pruning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "npSFb7QQ5skR",
    "outputId": "41dd3b96-df66-4d4b-ce49-9eb5cf2f908d"
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "structured_prune_mlp(model, keep_ratio=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCyWsTdkBJkA"
   },
   "source": [
    "**í”„ë£¨ë‹ëœ ëª¨ë¸ ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af4qHFsf5xeC",
    "outputId": "b07851bf-39ee-4d2e-b0a3-be0b3eece422"
   },
   "outputs": [],
   "source": [
    "quick_infer(model, tokenizer, \"Q: What is the ideal daytime temperature for tomato plants?\\nA\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29EZgOQ-6bBg"
   },
   "source": [
    "**í”„ë£¨ë‹ëœ ëª¨ë¸ì„ LoRA íŒŒì¸íŠœë‹**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0QJ0k5s8eVC"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"up_proj\", \"down_proj\", \"gate_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152,
     "referenced_widgets": [
      "eb2c31c90ca24a25b1a915ea696b5c36",
      "088cd9b4a9614c67a5d8cf09a591c918",
      "2ee0daed6cfa4fa599f0d94a7682e68f",
      "14401aa982ac41a5b1a2e0b737827082",
      "17bf99c847f9429295a306f08ef9289e",
      "21c65237a73445c3b135f627fc0cb9a1",
      "a4006cc232134a808edc75c46908e308",
      "9218aa7d2edc49239cd482600f0c3736",
      "6a19fd0f6ca44455910674584a1a1094",
      "f0fb80f3e46e400eb214fc60e8925953",
      "db3caa2ba89c4541b4b5cc5380d94fed",
      "1c4cb49e79f24fc3969679bde3c8a557",
      "bc04319d278044079900a108917ad480",
      "59f5c5b5a566449d96f72b0b0c20d598",
      "39620de5cda6432b87d5bdd60b3756dd",
      "0e66d0b71c0f4f058002204652a5d25c",
      "3e4464e8f1ff45baad618a79853c5cb9",
      "1bc6364db8804b42a5427aa0911e5e35",
      "63c8fe3f5cbf4afa91b8ef79de9235a9",
      "6902f1f7ade949ba8ba745b0c42ec59b",
      "7e590c09f8fa468d8e8e53b2bf23eba2",
      "a2b256dcd9c949f0b8eaa3b8486751c2",
      "23a2483bc25d4cd3b10e2b607419e1dd",
      "9a36990da2f4449e80f61fa930d0f247",
      "968cf7c6ffd6424ebff78d5bdfa9fc4a",
      "7e4cee3f694046dc9543fc9b930a4e71",
      "dc17c6fb0ff44756a2b502aecefb5bf3",
      "19891b19dc1c41a6a31b8c1373ea111e",
      "70d9a4121b1143a48c955a8693c9992b",
      "d2db2bdf367d47c9a19183e25ac63a78",
      "ce04730f15714ae4b2be2a6b3a3b116e",
      "cdc006fe68ac4d3b89ee86de98c4a77f",
      "b38bb8e0d9a643b29faa7b05f153f3e6"
     ]
    },
    "id": "5r9xw8WC87Sz",
    "outputId": "7c6e1f00-a98e-47b5-9dfb-7525483db84c"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# JSONL íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "dataset = load_dataset(\"json\", data_files=\"/content/tomato_qa_3000.jsonl\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)   # train:eval = 80:20\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset  = dataset[\"test\"]\n",
    "\n",
    "# í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ (batched=True ì§€ì›)\n",
    "def tokenize_fn(examples):\n",
    "    texts = [f\"Q: {ins}\\nA: {out} {tokenizer.eos_token}\"\n",
    "             for ins, out in zip(examples[\"instruction\"], examples[\"output\"])]\n",
    "    return tokenizer(texts, truncation=True, max_length=128, padding=\"max_length\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "eval_dataset  = eval_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "print(train_dataset[0])  # í† í¬ë‚˜ì´ì¦ˆ ê²°ê³¼ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "SAH1waFJ8irp",
    "outputId": "87bf27e0-3e77-4008-e483-278bc8b73ae3"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "\n",
    "wandb.login()\n",
    "wandb.init(project=\"tomato-qa\", name=\"tomato-qa-pruning-lora\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-tomato\",\n",
    "    logging_dir=\"./logs\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    eval_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=500,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"tomato-qa-pruning-lora\",\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"ğŸ”— ëŒ€ì‹œë³´ë“œ URL:\", wandb.run.get_url())\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0shu2WgV-W3B",
    "outputId": "fc8956fb-6e9b-4559-a5a2-b3c74f9c6d35"
   },
   "outputs": [],
   "source": [
    "quick_infer(model, tokenizer, \"Q: What is the ideal daytime temperature for tomato plants?\\nA\", max_new_tokens=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVZ5MP1XBRf6"
   },
   "source": [
    "**êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì‚¬ë³¸ ì €ì¥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "coJ8BHF9-oIT",
    "outputId": "c5ef5e31-45f0-46c2-c7c0-9fdb6ad14076"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp -r ./lora-tomato /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
