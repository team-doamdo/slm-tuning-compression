# LoRA ê¸°ë°˜ í”„ë£¨ë‹ íŒŒì´í”„ë¼ì¸

import os
import torch
import json
from pathlib import Path
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
import torch.nn.utils.prune as prune

# ì„¤ì •
MODEL_NAME = "google/gemma-3-1b-pt"
DATA_PATH = "./tomato_qa_3000.jsonl"
LORA_DIR = "./lora_finetuned"
PRUNED_LORA_DIR = "./pruned_lora"
RECOVERED_DIR = "./recovered_model"

class LoRAFirstPipeline:
    def __init__(self):
        # MPS ìš°ì„  ì‚¬ìš©í•˜ë˜ ì•ˆì •ì„± ì²´í¬
        if torch.backends.mps.is_available():
            self.device = "mps"
            print(f"Using device: {self.device} (Apple Silicon GPU)")
        else:
            self.device = "cpu"
            print(f"Using device: {self.device}")
        
        # MPS ìµœì í™” ì„¤ì •
        if self.device == "mps":
            torch.mps.empty_cache()
            # MPSì—ì„œ ì•ˆì •ì„±ì„ ìœ„í•œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            import os
            os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        
    def step1_lora_finetune(self, max_samples=100):
        """1ë‹¨ê³„: LoRA íŒŒì¸íŠœë‹"""
        print("="*50)
        print("1ï¸âƒ£ STEP 1: LoRA Fine-tuning")
        print("="*50)
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        print("ğŸ“¥ Loading base Gemma model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,
            trust_remote_code=True,
            attn_implementation="eager",
        ).to(self.device)
        
        # LoRA ì„¤ì • (ë” ê°•ë ¥í•˜ê²Œ)
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # rank ì¦ê°€ (8 -> 16)
            lora_alpha=32,  # alphaë„ ë¹„ë¡€ì ìœ¼ë¡œ ì¦ê°€
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
        )
        
        # LoRA ëª¨ë¸ ìƒì„±
        model = get_peft_model(base_model, lora_config)
        print(f"ğŸ“Š Trainable parameters: {model.num_parameters():,}")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = load_dataset("json", data_files=DATA_PATH, split="train")
        if max_samples < len(dataset):
            import random
            indices = random.sample(range(len(dataset)), max_samples)
            dataset = dataset.select(indices)
            print(f"ğŸ“Š Using {len(dataset)} samples")
        
        def preprocess(examples):
            instructions = examples.get("instruction", [])
            outputs = examples.get("output", [])
            texts = [f"Question: {q} Answer: {a}" for q, a in zip(instructions, outputs)]
            
            tokenized = tokenizer(
                texts, 
                truncation=True, 
                max_length=128, 
                padding="max_length",  # íŒ¨ë”© ì¶”ê°€
                return_tensors=None
            )
            # labelsë¥¼ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§ì¶¤
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
            
        tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)
        
        # LoRA í›ˆë ¨ ì„¤ì • (GPU ìµœì í™”)
        training_args = TrainingArguments(
            output_dir=LORA_DIR,
            overwrite_output_dir=True,
            per_device_train_batch_size=4 if self.device == "mps" else 1,  # MPSì—ì„œ ë°°ì¹˜ ì¦ê°€
            gradient_accumulation_steps=2 if self.device == "mps" else 4,
            num_train_epochs=8,  # ë” ë§ì€ ì—í­
            learning_rate=1e-3,  # ë” ë†’ì€ í•™ìŠµë¥ 
            save_strategy="epoch",
            logging_steps=2,
            eval_strategy="no",
            report_to=[],
            dataloader_num_workers=0,
            warmup_steps=10,
            weight_decay=0.01,
            dataloader_pin_memory=False,  # MPS í˜¸í™˜ì„±
            fp16=False,  # MPSì—ì„œëŠ” fp16 ë¹„í™œì„±í™”
            gradient_checkpointing=False,  # ì•ˆì •ì„±ì„ ìœ„í•´ ë¹„í™œì„±í™”
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            tokenizer=tokenizer,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        )
        
        print("âš¡ Starting LoRA fine-tuning...")
        trainer.train()
        
        # LoRA ì–´ëŒ‘í„° ì €ì¥
        model.save_pretrained(LORA_DIR)
        tokenizer.save_pretrained(LORA_DIR)
        print(f"âœ… LoRA fine-tuning complete! Saved to {LORA_DIR}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, base_model, trainer
        
    def step2_lora_pruning(self):
        """2ë‹¨ê³„: ì ì§„ì  LoRA í”„ë£¨ë‹ (2-3%ì”©)"""
        print("="*50)
        print("2ï¸âƒ£ STEP 2: Fine-grained LoRA Pruning")
        print("="*50)
        
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,
            trust_remote_code=True,
        ).to(self.device)
        
        # LoRA ëª¨ë¸ ë¡œë“œ
        model = PeftModel.from_pretrained(base_model, LORA_DIR)
        
        # ì„¸ë°€í•œ ì ì§„ì  í”„ë£¨ë‹ (2-3%ì”©)
        pruning_percentages = [2, 5, 8, 12, 15, 20]  # ë§¤ìš° ì ì§„ì 
        
        for percentage in pruning_percentages:
            print(f"\nâœ‚ï¸ Applying {percentage}% pruning...")
            
            # rank ê°ì†Œë¥¼ ë” ì„¸ë°€í•˜ê²Œ
            reduction_factor = percentage / 100.0
            new_rank = max(2, int(8 * (1 - reduction_factor)))  # ìµœì†Œ rank 2
            
            print(f"   ğŸ“Š Rank: 8 â†’ {new_rank} ({reduction_factor*100:.0f}% reduction)")
            
            # ìƒˆë¡œìš´ LoRA ì„¤ì •
            pruned_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=new_rank,
                lora_alpha=16,
                lora_dropout=0.1,
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
            )
            
            # ìƒˆ ëª¨ë¸ ìƒì„±
            fresh_base = AutoModelForCausalLM.from_pretrained(
                MODEL_NAME,
                torch_dtype=torch.float32,
                trust_remote_code=True,
            ).to(self.device)
            
            pruned_model = get_peft_model(fresh_base, pruned_config)
            
            # í”„ë£¨ë‹ëœ ëª¨ë¸ ì €ì¥
            stage_dir = Path(PRUNED_LORA_DIR) / f"pruned_{percentage}"
            stage_dir.mkdir(parents=True, exist_ok=True)
            
            pruned_model.save_pretrained(stage_dir)
            
            # í† í¬ë‚˜ì´ì €ë„ í•¨ê»˜ ì €ì¥ (í‰ê°€ì‹œ í•„ìš”)
            tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, trust_remote_code=True)
            tokenizer.save_pretrained(stage_dir)
            
            # í”„ë£¨ë‹ ì •ë³´ ì €ì¥
            pruning_info = {
                "pruning_percentage": percentage,
                "original_rank": 8,
                "pruned_rank": new_rank,
                "compression_ratio": f"{8/new_rank:.2f}x"
            }
            
            with open(stage_dir / "pruning_info.json", "w") as f:
                json.dump(pruning_info, f, indent=2)
                
            print(f"   ğŸ’¾ Saved to {stage_dir}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ í‰ê°€
            del pruned_model, fresh_base
            
            # ê° í”„ë£¨ë‹ ë‹¨ê³„ í›„ ì¦‰ì‹œ í‰ê°€ (ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ)
            print(f"   ğŸ“Š Evaluating {percentage}% pruned model...")
            try:
                # ê°„ë‹¨í•œ í‰ê°€ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´)
                eval_result = self._quick_evaluate(f"Pruned {percentage}%", stage_dir)
                print(f"   ğŸ“Š Quick evaluation: {eval_result:.1f}% accuracy")
            except Exception as e:
                print(f"   âŒ Quick evaluation failed: {e}")
                
        print("\nâœ… Fine-grained LoRA pruning complete!")
        
        del model, base_model
        
    def step3_recovery_training(self):
        """3ë‹¨ê³„: ë³µêµ¬ í›ˆë ¨"""
        print("="*50)
        print("3ï¸âƒ£ STEP 3: Recovery Training")
        print("="*50)
        
        # ê°€ì¥ ë§ì´ í”„ë£¨ë‹ëœ ëª¨ë¸ ì„ íƒ (20%)
        pruned_path = Path(PRUNED_LORA_DIR) / "pruned_20"
        
        if not pruned_path.exists():
            print("âŒ Pruned model not found. Run step 2 first.")
            return
            
        # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float32,
            trust_remote_code=True,
        ).to(self.device)
        
        # í”„ë£¨ë‹ëœ LoRA ëª¨ë¸ ë¡œë“œ
        model = PeftModel.from_pretrained(base_model, pruned_path)
        
        # ë³µêµ¬ìš© ë°ì´í„°ì…‹
        dataset = load_dataset("json", data_files=DATA_PATH, split="train")
        dataset = dataset.select(range(30))  # ì‘ì€ ë°ì´í„°ì…‹
        
        tokenizer = AutoTokenizer.from_pretrained(LORA_DIR, trust_remote_code=True)
        
        def preprocess(examples):
            instructions = examples.get("instruction", [])
            outputs = examples.get("output", [])
            texts = [f"Question: {q} Answer: {a}" for q, a in zip(instructions, outputs)]
            
            tokenized = tokenizer(
                texts, 
                truncation=True, 
                max_length=128, 
                padding="max_length",
                return_tensors=None
            )
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
            
        tokenized_dataset = dataset.map(preprocess, batched=True, remove_columns=dataset.column_names)
        
        # ë³µêµ¬ í›ˆë ¨ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=RECOVERED_DIR,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=2,
            num_train_epochs=2,
            learning_rate=5e-5,
            save_strategy="epoch",
            logging_steps=5,
            eval_strategy="no",
            report_to=[],
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            tokenizer=tokenizer,
            data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        )
        
        print("âš¡ Starting recovery training...")
        trainer.train()
        model.save_pretrained(RECOVERED_DIR)
        tokenizer.save_pretrained(RECOVERED_DIR)
        print("âœ… Recovery complete!")
        
        # ë³µêµ¬ í›„ í‰ê°€
        print("\nğŸ“Š Evaluating after recovery training...")
        self._evaluate_model("LoRA Recovered", MODEL_NAME, RECOVERED_DIR)
        
        del model, base_model, trainer
        
    def step4_compare_models(self):
        """4ë‹¨ê³„: ëª¨ë¸ ë¹„êµ"""
        print("="*50)
        print("4ï¸âƒ£ STEP 4: Model Comparison")
        print("="*50)
        
        test_questions = [
            "What is the optimal temperature for tomato cultivation?",
            "How should tomatoes be watered?",
            "What soil pH is best for tomatoes?"
        ]
        
        models_to_test = [
            ("Original", MODEL_NAME, None),
            ("LoRA Fine-tuned", MODEL_NAME, LORA_DIR),
            ("LoRA Pruned 30%", MODEL_NAME, Path(PRUNED_LORA_DIR) / "pruned_30"),
            ("Recovered", MODEL_NAME, RECOVERED_DIR),
        ]
        
        results = {}
        
        for model_name, base_path, lora_path in models_to_test:
            print(f"\nğŸ§ª Testing {model_name}...")
            try:
                tokenizer = AutoTokenizer.from_pretrained(
                    lora_path or base_path, trust_remote_code=True
                )
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                
                base_model = AutoModelForCausalLM.from_pretrained(
                    base_path,
                    torch_dtype=torch.float32,
                    trust_remote_code=True,
                ).to(self.device)
                
                if lora_path and Path(lora_path).exists():
                    model = PeftModel.from_pretrained(base_model, lora_path)
                else:
                    model = base_model
                    
                model.eval()
                
                model_results = []
                for question in test_questions:
                    prompt = f"Question: {question} Answer:"
                    inputs = tokenizer(prompt, return_tensors="pt")
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=30,
                            do_sample=False,
                            pad_token_id=tokenizer.eos_token_id
                        )
                    
                    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    clean_answer = answer[len(prompt):].strip()
                    model_results.append(clean_answer)
                    
                results[model_name] = model_results
                print(f"âœ… {model_name} tested")
                
                del model
                if lora_path is None:
                    del base_model
                    
            except Exception as e:
                print(f"âŒ {model_name} failed: {e}")
                results[model_name] = ["Error"] * len(test_questions)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("ğŸ“Š COMPARISON RESULTS")
        print("="*80)
        
        for i, question in enumerate(test_questions):
            print(f"\n[Question {i+1}] {question}")
            print("-" * 60)
            for model_name in results:
                answer = results[model_name][i]
                if len(answer) > 50:
                    answer = answer[:47] + "..."
                print(f"{model_name:<20}: {answer}")
                
        # ê²°ê³¼ ì €ì¥
        with open("lora_results.json", "w") as f:
            json.dump({"questions": test_questions, "results": results}, f, indent=2)
            
    def _evaluate_model(self, model_name, base_path, lora_path):
        """ê° ë‹¨ê³„ë³„ ëª¨ë¸ ì •í™•ë„ í‰ê°€ (ì‹œê°„/ë©”ëª¨ë¦¬ ì¸¡ì • í¬í•¨)"""
        import time
        import psutil
        import os
        
        # í‘œì¤€ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤ (ììœ  ë‹µë³€)
        test_cases = [
            "What are symptoms of phosphorus deficiency in tomatoes?",
            "What is the optimal temperature for tomato cultivation?", 
            "How should tomatoes be watered to avoid blossom-end rot?",
            "What soil pH is best for tomatoes?"
        ]
        
        try:
            # ì‹œì‘ ì‹œê°„ ë° ë©”ëª¨ë¦¬ ì¸¡ì •
            start_time = time.time()
            process = psutil.Process()
            start_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # ëª¨ë¸ ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                lora_path or base_path, trust_remote_code=True
            )
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                
            base_model = AutoModelForCausalLM.from_pretrained(
                base_path,
                torch_dtype=torch.float32,
                trust_remote_code=True,
            ).to(self.device)
            
            if lora_path and Path(lora_path).exists():
                model = PeftModel.from_pretrained(base_model, lora_path)
            else:
                model = base_model
                
            model.eval()
            
            # ë¡œë”© í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
            load_memory = process.memory_info().rss / 1024 / 1024  # MB
            load_time = time.time() - start_time
            
            # ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
            correct_answers = 0
            total_questions = len(test_cases)
            total_inference_time = 0
            
            print(f"\nğŸ“‹ Testing {model_name}:")
            print(f"   ğŸ”§ Load time: {load_time:.2f}s | Memory used: {load_memory - start_memory:.1f}MB")
            print("-" * 60)
            
            all_answers = []
            
            for i, question in enumerate(test_cases, 1):
                prompt = f"Question: {question} Answer:"
                inputs = tokenizer(prompt, return_tensors="pt")
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                inference_start = time.time()
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=80,  # ë” ê¸´ ë‹µë³€ í—ˆìš©
                        do_sample=True,     # ë” ë‹¤ì–‘í•œ ë‹µë³€ ìƒì„±
                        temperature=0.7,    # ì ë‹¹í•œ ì°½ì˜ì„±
                        top_p=0.9,
                        pad_token_id=tokenizer.eos_token_id
                    )
                inference_time = time.time() - inference_start
                total_inference_time += inference_time
                
                # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì¸¡ì •
                current_memory = process.memory_info().rss / 1024 / 1024  # MB
                
                answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
                clean_answer = answer[len(prompt):].strip()
                
                # ë‹µë³€ ì €ì¥ (ë‚˜ì¤‘ì— ë¹„êµìš©)
                all_answers.append(clean_answer)
                
                print(f"[{i}] {question}")
                print(f"    Answer: {clean_answer}")
                print(f"    â±ï¸ Time: {inference_time:.2f}s | ğŸ’¾ Memory: {current_memory:.0f}MB")
                print()
                
            # ì „ì²´ í†µê³„
            avg_inference_time = total_inference_time / total_questions
            peak_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            print(f"ğŸ“Š {model_name} Performance Summary:")
            print(f"    Total questions: {total_questions}")
            print(f"    Avg inference time: {avg_inference_time:.2f}s")
            print(f"    Peak memory usage: {peak_memory:.0f}MB")
            
            # ê²°ê³¼ë¥¼ ê¸€ë¡œë²Œ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
            if not hasattr(self, 'evaluation_results'):
                self.evaluation_results = []
                
            self.evaluation_results.append({
                "model_name": model_name,
                "answers": all_answers,
                "questions": test_cases,
                "load_time": load_time,
                "avg_inference_time": avg_inference_time,
                "peak_memory_mb": peak_memory,
                "memory_increase_mb": peak_memory - start_memory
            })
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del model
            if lora_path is None:
                del base_model
                
        except Exception as e:
            print(f"âŒ Evaluation failed for {model_name}: {e}")
            if not hasattr(self, 'evaluation_results'):
                self.evaluation_results = []
            self.evaluation_results.append({
                "model_name": model_name,
                "accuracy": 0.0,
                "error": str(e)
            })
        
    def run_full_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸŒ± LoRA Pipeline Started!")
        
        try:
            self.step1_lora_finetune()
            self.step2_lora_pruning()
            self.step3_recovery_training()
            self.step4_compare_models()
            
            print("\nğŸ‰ PIPELINE COMPLETE!")
            
        except Exception as e:
            print(f"\nâŒ Pipeline failed: {e}")
            raise

def main():
    import sys
    
    pipeline = LoRAFirstPipeline()
    
    if len(sys.argv) > 1:
        step = sys.argv[1]
        if step == "1":
            max_samples = int(sys.argv[2]) if len(sys.argv) > 2 else 100
            pipeline.step1_lora_finetune(max_samples=max_samples)
        elif step == "2":
            pipeline.step2_lora_pruning()
        elif step == "3":
            pipeline.step3_recovery_training()
        elif step == "4":
            pipeline.step4_compare_models()
        elif step == "eval":
            # í‰ê°€ë§Œ ë”°ë¡œ ì‹¤í–‰
            if len(sys.argv) > 2:
                eval_type = sys.argv[2]
                if eval_type == "original":
                    print("Evaluating original Gemma model...")
                    pipeline._evaluate_model("Original Gemma", MODEL_NAME, None)
                elif eval_type == "lora":
                    print("Evaluating LoRA fine-tuned model...")
                    pipeline._evaluate_model("LoRA Fine-tuned", MODEL_NAME, LORA_DIR)
                elif eval_type == "pruned":
                    percentage = sys.argv[3] if len(sys.argv) > 3 else "20"
                    path = Path(PRUNED_LORA_DIR) / f"pruned_{percentage}"
                    print(f"Evaluating {percentage}% pruned model...")
                    pipeline._evaluate_model(f"Pruned {percentage}%", MODEL_NAME, str(path))
                elif eval_type == "recovered":
                    print("Evaluating recovered model...")
                    pipeline._evaluate_model("Recovered", MODEL_NAME, RECOVERED_DIR)
                elif eval_type == "all":
                    print("Evaluating all available models...")
                    # ì›ë³¸ ëª¨ë¸
                    pipeline._evaluate_model("Original Gemma", MODEL_NAME, None)
                    
                    # LoRA ëª¨ë¸ (ìˆìœ¼ë©´)
                    if Path(LORA_DIR).exists():
                        pipeline._evaluate_model("LoRA Fine-tuned", MODEL_NAME, LORA_DIR)
                    
                    # í”„ë£¨ë‹ëœ ëª¨ë¸ë“¤ (ìˆìœ¼ë©´)
                    for percentage in [2, 5, 8, 12, 15, 20]:
                        path = Path(PRUNED_LORA_DIR) / f"pruned_{percentage}"
                        if path.exists():
                            pipeline._evaluate_model(f"Pruned {percentage}%", MODEL_NAME, str(path))
                    
                    # ë³µêµ¬ëœ ëª¨ë¸ (ìˆìœ¼ë©´)
                    if Path(RECOVERED_DIR).exists():
                        pipeline._evaluate_model("Recovered", MODEL_NAME, RECOVERED_DIR)
                else:
                    print("Invalid eval type. Use: original, lora, pruned [%], recovered, all")
            else:
                print("Usage: eval [original|lora|pruned [%]|recovered|all]")
                print("Examples:")
                print("  eval lora           - Evaluate LoRA model")
                print("  eval pruned 15      - Evaluate 15% pruned model") 
                print("  eval all            - Evaluate all available models")
        else:
            print("Invalid step. Use: 1, 2, 3, 4, eval")
    else:
        pipeline.run_full_pipeline()

if __name__ == "__main__":
    main()
